{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Caption Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HurleyJames/NNs/blob/master/Image_Caption_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a31sFIyrHaXl",
        "colab_type": "text"
      },
      "source": [
        "# Image Caption Generation\n",
        "\n",
        "Starter code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81kdnnwJvTFx",
        "colab_type": "text"
      },
      "source": [
        "## Text preparation \n",
        "\n",
        "We need to build a vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-C_TDN97yX_",
        "colab_type": "code",
        "outputId": "45e6cfb8-0274-48d2-fa1b-7170eedc9654",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXpWOFqFOXcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mounted Drive if using Colab; otherwise, your local path\n",
        "root = \"drive/My Drive/Flickr8k/\" # <--- replace this with your root data directory\n",
        "caption_dir = root + \"captions/\"                       # <--- replace these too\n",
        "image_dir = root + \"images/\"                           # <---\n",
        "\n",
        "\n",
        "token_file = \"Flickr8k.token.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9AkORttFoF_",
        "colab_type": "text"
      },
      "source": [
        "A helper function to read in our ground truth text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHC0y7zaOXq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_lines(filepath):\n",
        "    \"\"\" Open the ground truth captions into memory, line by line. \"\"\"\n",
        "    file = open(filepath, 'r')\n",
        "    lines = []\n",
        "\n",
        "    while True: \n",
        "        # Get next line from file until there's no more\n",
        "        line = file.readline() \n",
        "        if not line: \n",
        "            break\n",
        "        lines.append(line.strip())\n",
        "    file.close() \n",
        "    return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D86cJx2yv81K",
        "colab_type": "text"
      },
      "source": [
        "You can read all the ground truth captions (5 per image), into memory as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m-snsM2XHuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = read_lines(caption_dir + token_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IkK91ZuXNB2",
        "colab_type": "code",
        "outputId": "76a66d7b-5a77-41b9-a5ec-a58c183f2d58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "lines[:5]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .',\n",
              " '1000268201_693b08cb0e.jpg#1\\tA girl going into a wooden building .',\n",
              " '1000268201_693b08cb0e.jpg#2\\tA little girl climbing into a wooden playhouse .',\n",
              " '1000268201_693b08cb0e.jpg#3\\tA little girl climbing the stairs to her playhouse .',\n",
              " '1000268201_693b08cb0e.jpg#4\\tA little girl in a pink dress going into a wooden cabin .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c_6V2LMUb1q",
        "colab_type": "code",
        "outputId": "5c6234f2-bf21-4f31-de0c-e05f864477c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(lines)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40455"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oksUJjLPwApA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\"Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n",
        "    def __init__(self):\n",
        "        # Intially, set both the IDs and words to empty dictionaries.\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        # If the word does not already exist in the dictionary, add it\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            # Increment the ID for the next word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        # If we try to access a word in the dictionary which does not exist, return the <unk> id\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEQtthpXwEoY",
        "colab_type": "text"
      },
      "source": [
        "Extract all the words from ```lines```, and create a list of them in a variable ```words```, for example:\n",
        "\n",
        "```words = [\"a\", \"an\", \"the\", \"cat\"... ]```\n",
        "\n",
        "No need to worry about duplicates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IsstzDhbvdb",
        "colab_type": "text"
      },
      "source": [
        "splitting the image ID from the caption text and split the caption text into words and trim any trailing whitespaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9M3UWSAwAsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "image_ids = []\n",
        "captions = []\n",
        "\n",
        "for i in lines:\n",
        "    image_ids.append(i.split('\\t')[0][:-6])\n",
        "    i = i.split('\\t')[1].replace(\" .\", \".\").lower()\n",
        "    i = re.sub(\"[{}]+\".format(string.punctuation), \"\", i)\n",
        "    captions.append(i.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8K3PtGlCJ4e",
        "colab_type": "code",
        "outputId": "0ff05b36-1316-4f65-f9c4-f015aa39547f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "captions[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'child',\n",
              " 'in',\n",
              " 'a',\n",
              " 'pink',\n",
              " 'dress',\n",
              " 'is',\n",
              " 'climbing',\n",
              " 'up',\n",
              " 'a',\n",
              " 'set',\n",
              " 'of',\n",
              " 'stairs',\n",
              " 'in',\n",
              " 'an',\n",
              " 'entry',\n",
              " 'way']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h32EvDjxngj1",
        "colab": {}
      },
      "source": [
        "total = sum(captions, [])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItTv04EMndZJ",
        "colab_type": "code",
        "outputId": "4dba0edf-1e98-412e-f712-7087bf5946e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "total[:5]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'child', 'in', 'a', 'pink']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSE8Awm6-S9W",
        "colab_type": "code",
        "outputId": "aaa7b5d6-425c-4497-a617-731cb63ae584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(total)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "436505"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJF1JovK5KCV",
        "colab_type": "code",
        "outputId": "748330a2-0b40-4b33-9ceb-08dd6f01805a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "word = []\n",
        "# c = Counter((map(tuple, captions)))\n",
        "c = Counter(total)\n",
        "# print(len(list(c.keys())))\n",
        "# print(len(list(c.values())))\n",
        "# print(len(c))\n",
        "print(list(c.keys())[1])\n",
        "for j in range(len(c)):\n",
        "    if (list(c.values())[j] > 3):\n",
        "        word.append(list(c.keys())[j])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "child\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dT8Q-8O-4aY",
        "colab_type": "code",
        "outputId": "99500a7a-1e18-45ab-f425-5aa0d1d9e1b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(word)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3436"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHBMe-ATwLIQ",
        "colab_type": "text"
      },
      "source": [
        "Build the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctwErx_ZwAzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a vocab instance\n",
        "vocab = Vocabulary()\n",
        "\n",
        "# Add the token words first\n",
        "vocab.add_word('<pad>')\n",
        "vocab.add_word('<start>')\n",
        "vocab.add_word('<end>')\n",
        "vocab.add_word('<unk>')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzEYIvJ-GA_G",
        "colab_type": "text"
      },
      "source": [
        "Add the rest of the words from the parsed captions:\n",
        "\n",
        "``` vocab.add_word('new_word')```\n",
        "\n",
        "Don't add words that appear three times or less."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj99JT2XwA4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in word:\n",
        "    vocab.add_word(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChA2jcHxX0Eg",
        "colab_type": "code",
        "outputId": "4303e0da-3283-4f31-a2a5-d772e75ee0f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3440"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB30f4wYwSvg",
        "colab_type": "text"
      },
      "source": [
        "## Dataset and loaders for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raEOHrpnbbKY",
        "colab_type": "text"
      },
      "source": [
        "Keeping the same order, concatenate all the cleaned words from each caption into a string again, and add them all to a list of strings ```cleaned_captions```. Store all the image ids in a list ```image_ids```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eKax9UJ1xPq",
        "colab_type": "code",
        "outputId": "8009cd89-c834-44f0-c860-c8aed3184e97",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "image_ids[29706]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3497238310_2abde3965d'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGGnaDIRbZUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_captions = []\n",
        "\n",
        "for i in range(len(captions)):\n",
        "    cleaned_captions.append(\" \".join(captions[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_FbII1VwVSg",
        "colab_type": "text"
      },
      "source": [
        "The dataframe for the image paths and captions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYQz4T3mwA2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'image_id': image_ids,\n",
        "    'path': [image_dir + image_id + \".jpg\" for image_id in image_ids],\n",
        "    'caption': cleaned_captions\n",
        "}\n",
        "\n",
        "data_df = pd.DataFrame(data, columns=['image_id', 'path', 'caption'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POB7UiJLwYsf",
        "colab_type": "code",
        "outputId": "34998a2e-94a5-4917-d267-4c7ccd423f36",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data_df.head(n=5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>path</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000268201_693b08cb0e</td>\n",
              "      <td>drive/My Drive/Flickr8k/images/1000268201_693b...</td>\n",
              "      <td>a child in a pink dress is climbing up a set o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000268201_693b08cb0e</td>\n",
              "      <td>drive/My Drive/Flickr8k/images/1000268201_693b...</td>\n",
              "      <td>a girl going into a wooden building</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000268201_693b08cb0e</td>\n",
              "      <td>drive/My Drive/Flickr8k/images/1000268201_693b...</td>\n",
              "      <td>a little girl climbing into a wooden playhouse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000268201_693b08cb0e</td>\n",
              "      <td>drive/My Drive/Flickr8k/images/1000268201_693b...</td>\n",
              "      <td>a little girl climbing the stairs to her playh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000268201_693b08cb0e</td>\n",
              "      <td>drive/My Drive/Flickr8k/images/1000268201_693b...</td>\n",
              "      <td>a little girl in a pink dress going into a woo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                image_id  ...                                            caption\n",
              "0  1000268201_693b08cb0e  ...  a child in a pink dress is climbing up a set o...\n",
              "1  1000268201_693b08cb0e  ...                a girl going into a wooden building\n",
              "2  1000268201_693b08cb0e  ...     a little girl climbing into a wooden playhouse\n",
              "3  1000268201_693b08cb0e  ...  a little girl climbing the stairs to her playh...\n",
              "4  1000268201_693b08cb0e  ...  a little girl in a pink dress going into a woo...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyF38cvvgA16",
        "colab_type": "code",
        "outputId": "0ec7c713-7727-49fa-a781-d6f33ceafcd8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cleaned_captions[1]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a girl going into a wooden building'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNLQ0K-_weJy",
        "colab_type": "text"
      },
      "source": [
        "This is the Flickr8k class for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqf2_F6YwakD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "from nltk import tokenize\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Flickr8k(Dataset):\n",
        "    \"\"\" Flickr8k custom dataset compatible with torch.utils.data.DataLoader. \"\"\"\n",
        "    \n",
        "    def __init__(self, df, vocab, transform=None):\n",
        "        \"\"\" Set the path for images, captions and vocabulary wrapper.\n",
        "        \n",
        "        Args:\n",
        "            df: df containing image paths and captions.\n",
        "            vocab: vocabulary wrapper.\n",
        "            transform: image transformer.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Returns one data pair (image and caption). \"\"\"\n",
        "\n",
        "        vocab = self.vocab\n",
        "\n",
        "        caption = self.df['caption'][index]\n",
        "        img_id = self.df['image_id'][index]\n",
        "        path = self.df['path'][index]\n",
        "\n",
        "        image = Image.open(open(path, 'rb'))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert caption (string) to word ids.\n",
        "        tokens = caption.split()\n",
        "        caption = []\n",
        "        # Build the Tensor version of the caption, with token words\n",
        "        caption.append(vocab('<start>'))\n",
        "        caption.extend([vocab(token) for token in tokens])\n",
        "        caption.append(vocab('<end>'))\n",
        "        target = torch.Tensor(caption)\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vkld_4CwkPO",
        "colab_type": "text"
      },
      "source": [
        "We need to overwrite the default PyTorch ```collate_fn()``` because our ground truth captions are sequential data of varying lengths. The default ```collate_fn()``` does not support merging the captions with padding.\n",
        "\n",
        "You can read more about it here: https://pytorch.org/docs/stable/data.html#dataloader-collate-fn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5YmKr9ewkqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def caption_collate_fn(data):\n",
        "    \"\"\" Creates mini-batch tensors from the list of tuples (image, caption).\n",
        "    Args:\n",
        "        data: list of tuple (image, caption). \n",
        "            - image: torch tensor of shape (3, 256, 256).\n",
        "            - caption: torch tensor of shape (?); variable length.\n",
        "    Returns:\n",
        "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
        "        targets: torch tensor of shape (batch_size, padded_length).\n",
        "        lengths: list; valid length for each padded caption.\n",
        "    \"\"\"\n",
        "    # Sort a data list by caption length from longest to shortest.\n",
        "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "    images, captions = zip(*data)\n",
        "\n",
        "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
        "    lengths = [len(cap) for cap in captions]\n",
        "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
        "    for i, cap in enumerate(captions):\n",
        "        end = lengths[i]\n",
        "        targets[i, :end] = cap[:end]        \n",
        "    return images, targets, lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6VDx2O5FSiM",
        "colab_type": "text"
      },
      "source": [
        "Now we define the data transform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpRbVk6BFTGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Crop size matches the input dimensions expected by the pre-trained ResNet\n",
        "data_transform = transforms.Compose([ \n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),  # Why do we choose 224 x 224?\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),   # Using ImageNet norms\n",
        "                         (0.229, 0.224, 0.225))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgS9OpZ7FaAj",
        "colab_type": "text"
      },
      "source": [
        "Initialising the datasets. The only twist is that every image has 5 ground truth captions, so each image appears five times in the dataframe. We don't want an image to appear in more than one set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnTvR684GGVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unit_size = 5\n",
        "\n",
        "train_split = 0.95 # Defines the ratio of train/test data.\n",
        "\n",
        "# We didn't shuffle the dataframe yet so this works\n",
        "train_size = unit_size * round(len(data_df)*train_split / unit_size)\n",
        "\n",
        "dataset_train = Flickr8k(\n",
        "    df=data_df[:train_size].reset_index(drop=True),\n",
        "    vocab=vocab,\n",
        "    transform=data_transform,\n",
        ")\n",
        "\n",
        "dataset_test = Flickr8k(\n",
        "    df=data_df[(train_size):].reset_index(drop=True),\n",
        "    vocab=vocab,\n",
        "    transform=data_transform,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWuWg72dGOq9",
        "colab_type": "text"
      },
      "source": [
        "Write the dataloaders ```train_loader``` and ```test_loader``` - explicitly replacing the collate_fn:\n",
        "\n",
        "```train_loader = torch.utils.data.DataLoader(\n",
        "  ...,\n",
        "  collate_fn=caption_collate_fn\n",
        ")```\n",
        "\n",
        "Set train batch size to 128 and be sure to set ```shuffle=True```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkNrIRbXGLFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    collate_fn=caption_collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    collate_fn=caption_collate_fn\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXlf8lt5TF0N",
        "colab_type": "text"
      },
      "source": [
        "## Encoder and decoder models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls8lyXA2GTC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True) # Pre-trained on ImageNet by default\n",
        "        layers = list(resnet.children())[:-1]      # Keep all layers except the last one\n",
        "        # Unpack the layers and create a new Sequential\n",
        "        self.resnet = nn.Sequential(*layers)\n",
        "        \n",
        "        # We want a specific output size, which is the size of our embedding, so\n",
        "        # we feed our extracted features from the last fc layer (dimensions 1 x 1000)\n",
        "        # into a Linear layer to resize\n",
        "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "        \n",
        "        # Batch normalisation helps to speed up training\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "        \n",
        "    def forward(self, images):\n",
        "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
        "        \n",
        "        # Complete graph here. Remember to put the ResNet layer in a with torch.no_grad() block\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "        # print(\"resnet_out:\", features.shape)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        # features = features.reshape(features.size(0), -1)\n",
        "        # print(\"reshape:\", features.shape)\n",
        "        features = self.linear(features)\n",
        "        # print(\"linear:\", features.shape)\n",
        "        features = self.bn(features)\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
        "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        # What is an embedding layer?\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # Define this layer (one at a time)\n",
        "        # self.lstm / self.rnn\n",
        "        self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        \n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.max_seq_length = max_seq_length\n",
        "        \n",
        "    def forward(self, features, captions, lengths):\n",
        "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "        # What is \"packing\" a padded sequence?\n",
        "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
        "\n",
        "        hiddens, _ = self.lstm(packed) # Replace with self.rnn when using RNN\n",
        "        outputs = self.linear(hiddens[0])\n",
        "        return outputs\n",
        "    \n",
        "    def sample(self, features, states=None):\n",
        "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
        "        sampled_ids = []\n",
        "        inputs = features.unsqueeze(1)\n",
        "        for i in range(self.max_seq_length):\n",
        "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
        "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
        "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
        "            sampled_ids.append(predicted)\n",
        "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
        "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
        "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
        "        return sampled_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9-qtkkMTrtB",
        "colab_type": "code",
        "outputId": "b9578864-8e66-42e1-b9d7-7fe65bd16508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhecFOMRUgpe",
        "colab_type": "text"
      },
      "source": [
        "Set training parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fd2-IX2Uer3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "learning_rate = 0.001\n",
        "# num_epochs = 1\n",
        "num_epochs = 5\n",
        "log_step = 10\n",
        "save_step = 1\n",
        "\n",
        "model_path= root + \"models/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlIwF6P8UgB4",
        "colab_type": "text"
      },
      "source": [
        "Initialize the models and set the learning parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxwDUlR2Uy7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Build the models\n",
        "encoder = EncoderCNN(embed_size).to(device)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimisation will be on the parameters of BOTH the enocder and decoder,\n",
        "# but excluding the ResNet parameters, only the new added layers.\n",
        "params = list(\n",
        "    decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters()\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2vw99JrmuT8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0a425877-751f-4f6d-cd03-9126979bd258"
      },
      "source": [
        "# if torch.cuda.is_available():\n",
        "#     encoder.cuda()\n",
        "#     decoder.cuda()\n",
        "#     print('CUDA activated.')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CUDA activated.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUmSb2MHEZw3",
        "colab_type": "text"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS4oN21vNKu7",
        "colab_type": "text"
      },
      "source": [
        "The loop to train the model. Feel free to put this in a function if you prefer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M7KY9G3NI8l",
        "colab_type": "code",
        "outputId": "df96aba0-e4ae-4ac2-c21f-6ddedfb753c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "# before training\n",
        "torch.save(encoder.state_dict(), os.path.join(model_path, 'encoder-0.ckpt'))\n",
        "torch.save(decoder.state_dict(), os.path.join(model_path, 'decoder-0.ckpt'))\n",
        "\n",
        "# Train the models\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, captions, lengths) in enumerate(train_loader):\n",
        "\n",
        "        # Set mini-batch dataset\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "\n",
        "        # Packed as well as we'll compare to the decoder outputs\n",
        "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
        "\n",
        "        # Forward, backward and optimize\n",
        "        features = encoder(images)\n",
        "        outputs = decoder(features, captions, lengths)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        # Zero gradients for both networks\n",
        "        decoder.zero_grad()\n",
        "        encoder.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print log info\n",
        "        if i % log_step == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                  .format(epoch, num_epochs, i, total_step, loss.item())) \n",
        "\n",
        "        # If you want to save the model checkpoints - recommended once you have everything working\n",
        "        # Make sure to save RNN and LSTM versions separately\n",
        "        # if (i+1) % save_step == 0:\n",
        "        #     torch.save(decoder.state_dict(), os.path.join(model_path, 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
        "        #     torch.save(encoder.state_dict(), os.path.join(model_path, 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
        "    torch.save(decoder.state_dict(), os.path.join(model_path, 'decoder-{}.ckpt'.format(epoch+1)))\n",
        "    torch.save(encoder.state_dict(), os.path.join(model_path, 'encoder-{}.ckpt'.format(epoch+1)))\n",
        "print('Finished')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0/5], Step [0/301], Loss: 8.1394\n",
            "Epoch [0/5], Step [10/301], Loss: 5.5855\n",
            "Epoch [0/5], Step [20/301], Loss: 5.1567\n",
            "Epoch [0/5], Step [30/301], Loss: 4.6634\n",
            "Epoch [0/5], Step [40/301], Loss: 4.4847\n",
            "Epoch [0/5], Step [50/301], Loss: 4.1518\n",
            "Epoch [0/5], Step [60/301], Loss: 4.0583\n",
            "Epoch [0/5], Step [70/301], Loss: 3.8681\n",
            "Epoch [0/5], Step [80/301], Loss: 3.8664\n",
            "Epoch [0/5], Step [90/301], Loss: 3.7695\n",
            "Epoch [0/5], Step [100/301], Loss: 3.5865\n",
            "Epoch [0/5], Step [110/301], Loss: 3.6665\n",
            "Epoch [0/5], Step [120/301], Loss: 3.7153\n",
            "Epoch [0/5], Step [130/301], Loss: 3.5131\n",
            "Epoch [0/5], Step [140/301], Loss: 3.5154\n",
            "Epoch [0/5], Step [150/301], Loss: 3.5484\n",
            "Epoch [0/5], Step [160/301], Loss: 3.2992\n",
            "Epoch [0/5], Step [170/301], Loss: 3.3849\n",
            "Epoch [0/5], Step [180/301], Loss: 3.3897\n",
            "Epoch [0/5], Step [190/301], Loss: 3.4679\n",
            "Epoch [0/5], Step [200/301], Loss: 3.3886\n",
            "Epoch [0/5], Step [210/301], Loss: 3.2217\n",
            "Epoch [0/5], Step [220/301], Loss: 3.2619\n",
            "Epoch [0/5], Step [230/301], Loss: 3.2948\n",
            "Epoch [0/5], Step [240/301], Loss: 3.2881\n",
            "Epoch [0/5], Step [250/301], Loss: 3.1039\n",
            "Epoch [0/5], Step [260/301], Loss: 3.2952\n",
            "Epoch [0/5], Step [270/301], Loss: 3.0644\n",
            "Epoch [0/5], Step [280/301], Loss: 3.2966\n",
            "Epoch [0/5], Step [290/301], Loss: 3.0731\n",
            "Epoch [0/5], Step [300/301], Loss: 3.2357\n",
            "Epoch [1/5], Step [0/301], Loss: 3.1049\n",
            "Epoch [1/5], Step [10/301], Loss: 3.0357\n",
            "Epoch [1/5], Step [20/301], Loss: 3.0656\n",
            "Epoch [1/5], Step [30/301], Loss: 2.9126\n",
            "Epoch [1/5], Step [40/301], Loss: 3.0063\n",
            "Epoch [1/5], Step [50/301], Loss: 2.9205\n",
            "Epoch [1/5], Step [60/301], Loss: 3.0894\n",
            "Epoch [1/5], Step [70/301], Loss: 3.0338\n",
            "Epoch [1/5], Step [80/301], Loss: 2.9494\n",
            "Epoch [1/5], Step [90/301], Loss: 2.8591\n",
            "Epoch [1/5], Step [100/301], Loss: 2.9757\n",
            "Epoch [1/5], Step [110/301], Loss: 2.9593\n",
            "Epoch [1/5], Step [120/301], Loss: 2.9046\n",
            "Epoch [1/5], Step [130/301], Loss: 2.8761\n",
            "Epoch [1/5], Step [140/301], Loss: 2.9182\n",
            "Epoch [1/5], Step [150/301], Loss: 2.9361\n",
            "Epoch [1/5], Step [160/301], Loss: 2.9060\n",
            "Epoch [1/5], Step [170/301], Loss: 2.8337\n",
            "Epoch [1/5], Step [180/301], Loss: 2.6873\n",
            "Epoch [1/5], Step [190/301], Loss: 3.0436\n",
            "Epoch [1/5], Step [200/301], Loss: 2.7882\n",
            "Epoch [1/5], Step [210/301], Loss: 2.6653\n",
            "Epoch [1/5], Step [220/301], Loss: 2.7416\n",
            "Epoch [1/5], Step [230/301], Loss: 2.6422\n",
            "Epoch [1/5], Step [240/301], Loss: 2.8714\n",
            "Epoch [1/5], Step [250/301], Loss: 2.8711\n",
            "Epoch [1/5], Step [260/301], Loss: 2.7415\n",
            "Epoch [1/5], Step [270/301], Loss: 2.6972\n",
            "Epoch [1/5], Step [280/301], Loss: 2.7545\n",
            "Epoch [1/5], Step [290/301], Loss: 2.8564\n",
            "Epoch [1/5], Step [300/301], Loss: 2.9429\n",
            "Epoch [2/5], Step [0/301], Loss: 2.6014\n",
            "Epoch [2/5], Step [10/301], Loss: 2.5263\n",
            "Epoch [2/5], Step [20/301], Loss: 2.5332\n",
            "Epoch [2/5], Step [30/301], Loss: 2.5819\n",
            "Epoch [2/5], Step [40/301], Loss: 2.6857\n",
            "Epoch [2/5], Step [50/301], Loss: 2.6388\n",
            "Epoch [2/5], Step [60/301], Loss: 2.6253\n",
            "Epoch [2/5], Step [70/301], Loss: 2.6714\n",
            "Epoch [2/5], Step [80/301], Loss: 2.6633\n",
            "Epoch [2/5], Step [90/301], Loss: 2.6580\n",
            "Epoch [2/5], Step [100/301], Loss: 2.5272\n",
            "Epoch [2/5], Step [110/301], Loss: 2.5553\n",
            "Epoch [2/5], Step [120/301], Loss: 2.5857\n",
            "Epoch [2/5], Step [130/301], Loss: 2.5856\n",
            "Epoch [2/5], Step [140/301], Loss: 2.6237\n",
            "Epoch [2/5], Step [150/301], Loss: 2.6263\n",
            "Epoch [2/5], Step [160/301], Loss: 2.6356\n",
            "Epoch [2/5], Step [170/301], Loss: 2.7692\n",
            "Epoch [2/5], Step [180/301], Loss: 2.5311\n",
            "Epoch [2/5], Step [190/301], Loss: 2.6090\n",
            "Epoch [2/5], Step [200/301], Loss: 2.6525\n",
            "Epoch [2/5], Step [210/301], Loss: 2.5498\n",
            "Epoch [2/5], Step [220/301], Loss: 2.7006\n",
            "Epoch [2/5], Step [230/301], Loss: 2.5538\n",
            "Epoch [2/5], Step [240/301], Loss: 2.6714\n",
            "Epoch [2/5], Step [250/301], Loss: 2.6085\n",
            "Epoch [2/5], Step [260/301], Loss: 2.6462\n",
            "Epoch [2/5], Step [270/301], Loss: 2.5308\n",
            "Epoch [2/5], Step [280/301], Loss: 2.5831\n",
            "Epoch [2/5], Step [290/301], Loss: 2.6315\n",
            "Epoch [2/5], Step [300/301], Loss: 2.5941\n",
            "Epoch [3/5], Step [0/301], Loss: 2.4596\n",
            "Epoch [3/5], Step [10/301], Loss: 2.5076\n",
            "Epoch [3/5], Step [20/301], Loss: 2.5221\n",
            "Epoch [3/5], Step [30/301], Loss: 2.4161\n",
            "Epoch [3/5], Step [40/301], Loss: 2.4324\n",
            "Epoch [3/5], Step [50/301], Loss: 2.4262\n",
            "Epoch [3/5], Step [60/301], Loss: 2.3792\n",
            "Epoch [3/5], Step [70/301], Loss: 2.4879\n",
            "Epoch [3/5], Step [80/301], Loss: 2.3664\n",
            "Epoch [3/5], Step [90/301], Loss: 2.3512\n",
            "Epoch [3/5], Step [100/301], Loss: 2.4264\n",
            "Epoch [3/5], Step [110/301], Loss: 2.4270\n",
            "Epoch [3/5], Step [120/301], Loss: 2.4290\n",
            "Epoch [3/5], Step [130/301], Loss: 2.4209\n",
            "Epoch [3/5], Step [140/301], Loss: 2.3269\n",
            "Epoch [3/5], Step [150/301], Loss: 2.4720\n",
            "Epoch [3/5], Step [160/301], Loss: 2.4122\n",
            "Epoch [3/5], Step [170/301], Loss: 2.3695\n",
            "Epoch [3/5], Step [180/301], Loss: 2.4011\n",
            "Epoch [3/5], Step [190/301], Loss: 2.4797\n",
            "Epoch [3/5], Step [200/301], Loss: 2.4115\n",
            "Epoch [3/5], Step [210/301], Loss: 2.4085\n",
            "Epoch [3/5], Step [220/301], Loss: 2.3851\n",
            "Epoch [3/5], Step [230/301], Loss: 2.3315\n",
            "Epoch [3/5], Step [240/301], Loss: 2.3459\n",
            "Epoch [3/5], Step [250/301], Loss: 2.4753\n",
            "Epoch [3/5], Step [260/301], Loss: 2.3637\n",
            "Epoch [3/5], Step [270/301], Loss: 2.5127\n",
            "Epoch [3/5], Step [280/301], Loss: 2.3500\n",
            "Epoch [3/5], Step [290/301], Loss: 2.4290\n",
            "Epoch [3/5], Step [300/301], Loss: 2.3766\n",
            "Epoch [4/5], Step [0/301], Loss: 2.3752\n",
            "Epoch [4/5], Step [10/301], Loss: 2.2489\n",
            "Epoch [4/5], Step [20/301], Loss: 2.3443\n",
            "Epoch [4/5], Step [30/301], Loss: 2.3564\n",
            "Epoch [4/5], Step [40/301], Loss: 2.3851\n",
            "Epoch [4/5], Step [50/301], Loss: 2.3155\n",
            "Epoch [4/5], Step [60/301], Loss: 2.2128\n",
            "Epoch [4/5], Step [70/301], Loss: 2.2435\n",
            "Epoch [4/5], Step [80/301], Loss: 2.2974\n",
            "Epoch [4/5], Step [90/301], Loss: 2.3061\n",
            "Epoch [4/5], Step [100/301], Loss: 2.3153\n",
            "Epoch [4/5], Step [110/301], Loss: 2.3843\n",
            "Epoch [4/5], Step [120/301], Loss: 2.2784\n",
            "Epoch [4/5], Step [130/301], Loss: 2.2733\n",
            "Epoch [4/5], Step [140/301], Loss: 2.2714\n",
            "Epoch [4/5], Step [150/301], Loss: 2.1616\n",
            "Epoch [4/5], Step [160/301], Loss: 2.3851\n",
            "Epoch [4/5], Step [170/301], Loss: 2.2688\n",
            "Epoch [4/5], Step [180/301], Loss: 2.3149\n",
            "Epoch [4/5], Step [190/301], Loss: 2.3150\n",
            "Epoch [4/5], Step [200/301], Loss: 2.2098\n",
            "Epoch [4/5], Step [210/301], Loss: 2.3242\n",
            "Epoch [4/5], Step [220/301], Loss: 2.3346\n",
            "Epoch [4/5], Step [230/301], Loss: 2.1808\n",
            "Epoch [4/5], Step [240/301], Loss: 2.3536\n",
            "Epoch [4/5], Step [250/301], Loss: 2.2691\n",
            "Epoch [4/5], Step [260/301], Loss: 2.1665\n",
            "Epoch [4/5], Step [270/301], Loss: 2.3481\n",
            "Epoch [4/5], Step [280/301], Loss: 2.2764\n",
            "Epoch [4/5], Step [290/301], Loss: 2.3902\n",
            "Epoch [4/5], Step [300/301], Loss: 2.0584\n",
            "Finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7HLBDrWJp8n",
        "colab_type": "text"
      },
      "source": [
        "## Random Interface from dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf2_gRdOJup4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "def getRandomImage(img_list):\n",
        "    \"\"\"Returns a random filename, choose among the files of the given path\"\"\"\n",
        "    index = random.randrange(0, len(img_list))\n",
        "    return img_list[index], index\n",
        "\n",
        "def loadImage(image_path, transform=None):\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize([224, 224])\n",
        "\n",
        "    if transform is not None:\n",
        "        image = transform(image).unsqueeze(0)\n",
        "\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nza_uE7PWnYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_list = []\n",
        "img_path = caption_dir + \"Flickr_8k.testImages.txt\"\n",
        "\n",
        "with open(img_path, 'r') as f:\n",
        "    for i in f:\n",
        "        img_list.append(i.strip())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFGsg30nAfBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# choose two random images for evaluation\n",
        "filename1, index1 = getRandomImage(img_list)\n",
        "filename2, index2 = getRandomImage(img_list)\n",
        "image1 = loadImage(image_dir + str(filename1), data_transform)\n",
        "image2 = loadImage(image_dir + str(filename2), data_transform) \n",
        "\n",
        "# image = Image.open(image_dir + str(filename1))\n",
        "# plt.imshow(np.asarray(image1))\n",
        "# plt.imshow(np.asarray(image2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onpfSoEZ-lRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image1_id = image_ids.index(filename1[:-4])\n",
        "image2_id = image_ids.index(filename2[:-4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUQ0mOQ5Mluu",
        "colab_type": "text"
      },
      "source": [
        "Loading checkpoint and initializing encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swQMgv1HMsQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch_checkpoint = ['0', '1', '2', '3', '4', '5']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-N3iX21j3n",
        "colab_type": "text"
      },
      "source": [
        "## BLEU for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFUJ3Bg62TOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def transform_idx_to_words(input):\n",
        "    sampled_caption = []\n",
        "\n",
        "    for idx in input:\n",
        "        word = vocab.idx2word[idx]\n",
        "        sampled_caption.append(word)\n",
        "\n",
        "        if word == '<end>':\n",
        "            break\n",
        "    \n",
        "    output = ' '.join(sampled_caption[1:-1])\n",
        "    output = output.replace(' ,', ',')\n",
        "    \n",
        "    return output.split(' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XucCmq4AXvH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "1e7ef5fd-210d-440c-f10c-9f72935a9b9b"
      },
      "source": [
        "bleu_score1 = []\n",
        "bleu_score2 = []\n",
        "# an image has 5 reference captions\n",
        "bleu_reference1 = []\n",
        "bleu_reference2 = []\n",
        "\n",
        "for model_name in range(len(epoch_checkpoint)):\n",
        "    print(model_name)\n",
        "    encoder_model_path = os.path.join(model_path, 'encoder-{}.ckpt'.format(model_name))\n",
        "    decoder_model_path = os.path.join(model_path, 'decoder-{}.ckpt'.format(model_name))\n",
        "\n",
        "    test_encoder = EncoderCNN(embed_size)\n",
        "    test_decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers)    \n",
        "\n",
        "    test_encoder.load_state_dict(torch.load(encoder_model_path))\n",
        "    test_decoder.load_state_dict(torch.load(decoder_model_path))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        test_encoder = test_encoder.to(device)\n",
        "        test_decoder = test_decoder.to(device)\n",
        "\n",
        "    test_encoder.eval()\n",
        "    test_decoder.eval()\n",
        "\n",
        "    image1 = image1.to(device)\n",
        "    image2 = image2.to(device)\n",
        "\n",
        "    predicted, actual = list(), list()\n",
        "\n",
        "    feature1 = test_encoder(image1)\n",
        "    cap1 = test_decoder.sample(feature1)\n",
        "    cap1 = cap1.cpu().data.numpy()\n",
        "    cap1 = cap1[0,:]\n",
        "\n",
        "    feature2 = test_encoder(image2)\n",
        "    cap2 = test_decoder.sample(feature2)\n",
        "    cap2 = cap2.cpu().data.numpy()\n",
        "    cap2 = cap2[0,:]\n",
        "\n",
        "    predicted.append(cap1)\n",
        "    predicted.append(cap2)\n",
        "\n",
        "    predicted1 = \" \".join(transform_idx_to_words(predicted[0]))\n",
        "    predicted2 = \" \".join(transform_idx_to_words(predicted[1]))\n",
        "\n",
        "    print(\"The information of 1st image:\" + filename1 + \"     \" + str(image1_id))\n",
        "    print(\"The generated caption of 1st image: \" + predicted1)\n",
        "\n",
        "    hypotheses = list()\n",
        "    references = list()\n",
        "\n",
        "    # 1st image\n",
        "    hypotheses.append(predicted1)\n",
        "    for i in range(len(epoch_checkpoint)):\n",
        "        references.append(cleaned_captions[image1_id + i])\n",
        "        print('The {} reference caption of 1st image: '.format(i) + references[0])\n",
        "        bleu = corpus_bleu(references, hypotheses, weights=(1.0/1.0,))\n",
        "        # TODO\n",
        "        # bleu4 = \n",
        "        print('BLEU score with {} reference caption: '.format(i) + str(bleu))\n",
        "        bleu_reference1.append(bleu)\n",
        "\n",
        "        references.clear();\n",
        "\n",
        "    # references.append(cleaned_captions[image1_id])\n",
        "    # print(\"The 1st reference caption of first image: \" + references[0])\n",
        "    # bleu = corpus_bleu(references, hypotheses, weights=(1.0/1.0,))\n",
        "    # print(\"BLEU score with 1st reference caption: \" + str(bleu))\n",
        "    # bleu_reference.append(bleu)\n",
        "\n",
        "    # references.clear()\n",
        "    # references.append(cleaned_captions[image1_id + 1])\n",
        "    # print(\"The 2nd reference caption of first image: \" + references[0])\n",
        "    # bleu = corpus_bleu(references, hypotheses, weights=(1.0/1.0,))\n",
        "    # print(\"BLEU score with 2nd reference caption: \" + str(bleu))\n",
        "    # bleu_reference.append(bleu)\n",
        "\n",
        "    # 2nd image\n",
        "    print(\"The information of 2nd image:\" + filename2 + \"     \" + str(image2_id))\n",
        "    print(\"The generated caption of 2nd image: \" + predicted2)\n",
        "\n",
        "    hypotheses.clear()\n",
        "    references.clear()\n",
        "\n",
        "    hypotheses.append(predicted2)\n",
        "    for i in range(len(epoch_checkpoint)):\n",
        "        references.append(cleaned_captions[image2_id + i])\n",
        "        print('The {} reference caption of 2nd image: '.format(i) + references[0])\n",
        "        bleu = corpus_bleu(references, hypotheses, weights=(1.0/1.0,))\n",
        "        print('BLEU score with {} reference caption: '.format(i) + str(bleu))\n",
        "        bleu_reference2.append(bleu)\n",
        "\n",
        "        references.clear();\n",
        "\n",
        "    bleu_score1.append((model_name, bleu_reference1))\n",
        "    bleu_score2.append((model_name, bleu_reference2))\n",
        "    print('done for model: {}'.format(model_name))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "The information of 1st image:3596131692_91b8a05606.jpg     32245\n",
            "The generated caption of 1st image: owner pigeons base carnival arched saxophone challenging extends collared brightlycolored zip player galloping training retriever retriever after casting\n",
            "The 0 reference caption of 1st image: children are playing soccer while an adult looks on\n",
            "BLEU score with 0 reference caption: 0.11111111111111109\n",
            "The 1 reference caption of 1st image: children dressed in green and yellow soccer uniforms run after a soccer ball\n",
            "BLEU score with 1 reference caption: 0.11764705882352941\n",
            "The 2 reference caption of 1st image: three children in neon colors play soccer while young man watches\n",
            "BLEU score with 2 reference caption: 0.11111111111111109\n",
            "The 3 reference caption of 1st image: three children wearing soccer uniforms chase after a blue and grey soccer ball\n",
            "BLEU score with 3 reference caption: 0.11764705882352941\n",
            "The 4 reference caption of 1st image: three child soccer players go for the ball\n",
            "BLEU score with 4 reference caption: 0.11111111111111109\n",
            "The 5 reference caption of 1st image: a jogger wears a robot hat\n",
            "BLEU score with 5 reference caption: 0.0718954248366013\n",
            "The information of 2nd image:3567061016_62768dcce1.jpg     31555\n",
            "The generated caption of 2nd image: kilt alligator elephant hooping thin overhang valley eggs and paintings looks door performers fresh photographers displays performers fresh\n",
            "The 0 reference caption of 2nd image: a bird flying in the air\n",
            "BLEU score with 0 reference caption: 0.0935251798561151\n",
            "The 1 reference caption of 2nd image: a bird flies low to the ground\n",
            "BLEU score with 1 reference caption: 0.10071942446043167\n",
            "The 2 reference caption of 2nd image: a bird with its wings spread\n",
            "BLEU score with 2 reference caption: 0.08633093525179855\n",
            "The 3 reference caption of 2nd image: a hawk flies down towards the grass\n",
            "BLEU score with 3 reference caption: 0.10791366906474818\n",
            "The 4 reference caption of 2nd image: a hawk is flying trailing lines from its legs\n",
            "BLEU score with 4 reference caption: 0.11510791366906477\n",
            "The 5 reference caption of 2nd image: a brown dog is splashing in a puddle in the grass\n",
            "BLEU score with 5 reference caption: 0.10071942446043167\n",
            "done for model: 0\n",
            "1\n",
            "The information of 1st image:3596131692_91b8a05606.jpg     32245\n",
            "The generated caption of 1st image: a young boy in a red shirt and a white shirt and a white dog\n",
            "The 0 reference caption of 1st image: children are playing soccer while an adult looks on\n",
            "BLEU score with 0 reference caption: 0.25\n",
            "The 1 reference caption of 1st image: children dressed in green and yellow soccer uniforms run after a soccer ball\n",
            "BLEU score with 1 reference caption: 0.26666666666666666\n",
            "The 2 reference caption of 1st image: three children in neon colors play soccer while young man watches\n",
            "BLEU score with 2 reference caption: 0.25\n",
            "The 3 reference caption of 1st image: three children wearing soccer uniforms chase after a blue and grey soccer ball\n",
            "BLEU score with 3 reference caption: 0.26666666666666666\n",
            "The 4 reference caption of 1st image: three child soccer players go for the ball\n",
            "BLEU score with 4 reference caption: 0.21666666666666667\n",
            "The 5 reference caption of 1st image: a jogger wears a robot hat\n",
            "BLEU score with 5 reference caption: 0.18333333333333332\n",
            "The information of 2nd image:3567061016_62768dcce1.jpg     31555\n",
            "The generated caption of 2nd image: a black dog is running through the snow\n",
            "The 0 reference caption of 2nd image: a bird flying in the air\n",
            "BLEU score with 0 reference caption: 0.30769230769230765\n",
            "The 1 reference caption of 2nd image: a bird flies low to the ground\n",
            "BLEU score with 1 reference caption: 0.41025641025641024\n",
            "The 2 reference caption of 2nd image: a bird with its wings spread\n",
            "BLEU score with 2 reference caption: 0.3333333333333333\n",
            "The 3 reference caption of 2nd image: a hawk flies down towards the grass\n",
            "BLEU score with 3 reference caption: 0.38461538461538464\n",
            "The 4 reference caption of 2nd image: a hawk is flying trailing lines from its legs\n",
            "BLEU score with 4 reference caption: 0.3589743589743589\n",
            "The 5 reference caption of 2nd image: a brown dog is splashing in a puddle in the grass\n",
            "BLEU score with 5 reference caption: 0.41025641025641024\n",
            "done for model: 1\n",
            "2\n",
            "The information of 1st image:3596131692_91b8a05606.jpg     32245\n",
            "The generated caption of 1st image: a young boy in a red shirt and a soccer ball in a field\n",
            "The 0 reference caption of 1st image: children are playing soccer while an adult looks on\n",
            "BLEU score with 0 reference caption: 0.2909090909090909\n",
            "The 1 reference caption of 1st image: children dressed in green and yellow soccer uniforms run after a soccer ball\n",
            "BLEU score with 1 reference caption: 0.32727272727272727\n",
            "The 2 reference caption of 1st image: three children in neon colors play soccer while young man watches\n",
            "BLEU score with 2 reference caption: 0.2909090909090909\n",
            "The 3 reference caption of 1st image: three children wearing soccer uniforms chase after a blue and grey soccer ball\n",
            "BLEU score with 3 reference caption: 0.32727272727272727\n",
            "The 4 reference caption of 1st image: three child soccer players go for the ball\n",
            "BLEU score with 4 reference caption: 0.2909090909090909\n",
            "The 5 reference caption of 1st image: a jogger wears a robot hat\n",
            "BLEU score with 5 reference caption: 0.18181818181818182\n",
            "The information of 2nd image:3567061016_62768dcce1.jpg     31555\n",
            "The generated caption of 2nd image: a black and white dog is running through the snow\n",
            "The 0 reference caption of 2nd image: a bird flying in the air\n",
            "BLEU score with 0 reference caption: 0.24489795918367346\n",
            "The 1 reference caption of 2nd image: a bird flies low to the ground\n",
            "BLEU score with 1 reference caption: 0.3265306122448979\n",
            "The 2 reference caption of 2nd image: a bird with its wings spread\n",
            "BLEU score with 2 reference caption: 0.2653061224489796\n",
            "The 3 reference caption of 2nd image: a hawk flies down towards the grass\n",
            "BLEU score with 3 reference caption: 0.3061224489795918\n",
            "The 4 reference caption of 2nd image: a hawk is flying trailing lines from its legs\n",
            "BLEU score with 4 reference caption: 0.2857142857142857\n",
            "The 5 reference caption of 2nd image: a brown dog is splashing in a puddle in the grass\n",
            "BLEU score with 5 reference caption: 0.3265306122448979\n",
            "done for model: 2\n",
            "3\n",
            "The information of 1st image:3596131692_91b8a05606.jpg     32245\n",
            "The generated caption of 1st image: two girls are playing soccer\n",
            "The 0 reference caption of 1st image: children are playing soccer while an adult looks on\n",
            "BLEU score with 0 reference caption: 0.5357142857142857\n",
            "The 1 reference caption of 1st image: children dressed in green and yellow soccer uniforms run after a soccer ball\n",
            "BLEU score with 1 reference caption: 0.5\n",
            "The 2 reference caption of 1st image: three children in neon colors play soccer while young man watches\n",
            "BLEU score with 2 reference caption: 0.5357142857142857\n",
            "The 3 reference caption of 1st image: three children wearing soccer uniforms chase after a blue and grey soccer ball\n",
            "BLEU score with 3 reference caption: 0.5\n",
            "The 4 reference caption of 1st image: three child soccer players go for the ball\n",
            "BLEU score with 4 reference caption: 0.4642857142857143\n",
            "The 5 reference caption of 1st image: a jogger wears a robot hat\n",
            "BLEU score with 5 reference caption: 0.32142857142857145\n",
            "The information of 2nd image:3567061016_62768dcce1.jpg     31555\n",
            "The generated caption of 2nd image: a bird flies through the air\n",
            "The 0 reference caption of 2nd image: a bird flying in the air\n",
            "BLEU score with 0 reference caption: 0.42857142857142855\n",
            "The 1 reference caption of 2nd image: a bird flies low to the ground\n",
            "BLEU score with 1 reference caption: 0.5357142857142857\n",
            "The 2 reference caption of 2nd image: a bird with its wings spread\n",
            "BLEU score with 2 reference caption: 0.39285714285714285\n",
            "The 3 reference caption of 2nd image: a hawk flies down towards the grass\n",
            "BLEU score with 3 reference caption: 0.4642857142857143\n",
            "The 4 reference caption of 2nd image: a hawk is flying trailing lines from its legs\n",
            "BLEU score with 4 reference caption: 0.42857142857142855\n",
            "The 5 reference caption of 2nd image: a brown dog is splashing in a puddle in the grass\n",
            "BLEU score with 5 reference caption: 0.5\n",
            "done for model: 3\n",
            "4\n",
            "The information of 1st image:3596131692_91b8a05606.jpg     32245\n",
            "The generated caption of 1st image: two boys playing soccer\n",
            "The 0 reference caption of 1st image: children are playing soccer while an adult looks on\n",
            "BLEU score with 0 reference caption: 0.6521739130434783\n",
            "The 1 reference caption of 1st image: children dressed in green and yellow soccer uniforms run after a soccer ball\n",
            "BLEU score with 1 reference caption: 0.6521739130434783\n",
            "The 2 reference caption of 1st image: three children in neon colors play soccer while young man watches\n",
            "BLEU score with 2 reference caption: 0.6521739130434783\n",
            "The 3 reference caption of 1st image: three children wearing soccer uniforms chase after a blue and grey soccer ball\n",
            "BLEU score with 3 reference caption: 0.6521739130434783\n",
            "The 4 reference caption of 1st image: three child soccer players go for the ball\n",
            "BLEU score with 4 reference caption: 0.6086956521739131\n",
            "The 5 reference caption of 1st image: a jogger wears a robot hat\n",
            "BLEU score with 5 reference caption: 0.43478260869565216\n",
            "The information of 2nd image:3567061016_62768dcce1.jpg     31555\n",
            "The generated caption of 2nd image: a bird flies through the air\n",
            "The 0 reference caption of 2nd image: a bird flying in the air\n",
            "BLEU score with 0 reference caption: 0.42857142857142855\n",
            "The 1 reference caption of 2nd image: a bird flies low to the ground\n",
            "BLEU score with 1 reference caption: 0.5357142857142857\n",
            "The 2 reference caption of 2nd image: a bird with its wings spread\n",
            "BLEU score with 2 reference caption: 0.39285714285714285\n",
            "The 3 reference caption of 2nd image: a hawk flies down towards the grass\n",
            "BLEU score with 3 reference caption: 0.4642857142857143\n",
            "The 4 reference caption of 2nd image: a hawk is flying trailing lines from its legs\n",
            "BLEU score with 4 reference caption: 0.42857142857142855\n",
            "The 5 reference caption of 2nd image: a brown dog is splashing in a puddle in the grass\n",
            "BLEU score with 5 reference caption: 0.5\n",
            "done for model: 4\n",
            "5\n",
            "The information of 1st image:3596131692_91b8a05606.jpg     32245\n",
            "The generated caption of 1st image: a boy in a red shirt is running on the grass\n",
            "The 0 reference caption of 1st image: children are playing soccer while an adult looks on\n",
            "BLEU score with 0 reference caption: 0.3181818181818182\n",
            "The 1 reference caption of 1st image: children dressed in green and yellow soccer uniforms run after a soccer ball\n",
            "BLEU score with 1 reference caption: 0.3409090909090909\n",
            "The 2 reference caption of 1st image: three children in neon colors play soccer while young man watches\n",
            "BLEU score with 2 reference caption: 0.3181818181818182\n",
            "The 3 reference caption of 1st image: three children wearing soccer uniforms chase after a blue and grey soccer ball\n",
            "BLEU score with 3 reference caption: 0.3409090909090909\n",
            "The 4 reference caption of 1st image: three child soccer players go for the ball\n",
            "BLEU score with 4 reference caption: 0.2954545454545455\n",
            "The 5 reference caption of 1st image: a jogger wears a robot hat\n",
            "BLEU score with 5 reference caption: 0.22727272727272727\n",
            "The information of 2nd image:3567061016_62768dcce1.jpg     31555\n",
            "The generated caption of 2nd image: a black and white dog is jumping over a hurdle\n",
            "The 0 reference caption of 2nd image: a bird flying in the air\n",
            "BLEU score with 0 reference caption: 0.2608695652173913\n",
            "The 1 reference caption of 2nd image: a bird flies low to the ground\n",
            "BLEU score with 1 reference caption: 0.3478260869565218\n",
            "The 2 reference caption of 2nd image: a bird with its wings spread\n",
            "BLEU score with 2 reference caption: 0.3043478260869566\n",
            "The 3 reference caption of 2nd image: a hawk flies down towards the grass\n",
            "BLEU score with 3 reference caption: 0.3260869565217391\n",
            "The 4 reference caption of 2nd image: a hawk is flying trailing lines from its legs\n",
            "BLEU score with 4 reference caption: 0.3260869565217391\n",
            "The 5 reference caption of 2nd image: a brown dog is splashing in a puddle in the grass\n",
            "BLEU score with 5 reference caption: 0.3695652173913043\n",
            "done for model: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-8p9J1aDkFB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2c8a444a-3ba3-4745-a2ba-afc685edd73b"
      },
      "source": [
        "bleu_score"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  [0.11111111111111109,\n",
              "   0.11764705882352941,\n",
              "   0.11111111111111109,\n",
              "   0.11764705882352941,\n",
              "   0.11111111111111109,\n",
              "   0.0718954248366013,\n",
              "   0.25,\n",
              "   0.26666666666666666,\n",
              "   0.25,\n",
              "   0.26666666666666666,\n",
              "   0.21666666666666667,\n",
              "   0.18333333333333332,\n",
              "   0.2909090909090909,\n",
              "   0.32727272727272727,\n",
              "   0.2909090909090909,\n",
              "   0.32727272727272727,\n",
              "   0.2909090909090909,\n",
              "   0.18181818181818182,\n",
              "   0.5357142857142857,\n",
              "   0.5,\n",
              "   0.5357142857142857,\n",
              "   0.5,\n",
              "   0.4642857142857143,\n",
              "   0.32142857142857145,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6086956521739131,\n",
              "   0.43478260869565216,\n",
              "   0.3181818181818182,\n",
              "   0.3409090909090909,\n",
              "   0.3181818181818182,\n",
              "   0.3409090909090909,\n",
              "   0.2954545454545455,\n",
              "   0.22727272727272727]),\n",
              " (1,\n",
              "  [0.11111111111111109,\n",
              "   0.11764705882352941,\n",
              "   0.11111111111111109,\n",
              "   0.11764705882352941,\n",
              "   0.11111111111111109,\n",
              "   0.0718954248366013,\n",
              "   0.25,\n",
              "   0.26666666666666666,\n",
              "   0.25,\n",
              "   0.26666666666666666,\n",
              "   0.21666666666666667,\n",
              "   0.18333333333333332,\n",
              "   0.2909090909090909,\n",
              "   0.32727272727272727,\n",
              "   0.2909090909090909,\n",
              "   0.32727272727272727,\n",
              "   0.2909090909090909,\n",
              "   0.18181818181818182,\n",
              "   0.5357142857142857,\n",
              "   0.5,\n",
              "   0.5357142857142857,\n",
              "   0.5,\n",
              "   0.4642857142857143,\n",
              "   0.32142857142857145,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6086956521739131,\n",
              "   0.43478260869565216,\n",
              "   0.3181818181818182,\n",
              "   0.3409090909090909,\n",
              "   0.3181818181818182,\n",
              "   0.3409090909090909,\n",
              "   0.2954545454545455,\n",
              "   0.22727272727272727]),\n",
              " (2,\n",
              "  [0.11111111111111109,\n",
              "   0.11764705882352941,\n",
              "   0.11111111111111109,\n",
              "   0.11764705882352941,\n",
              "   0.11111111111111109,\n",
              "   0.0718954248366013,\n",
              "   0.25,\n",
              "   0.26666666666666666,\n",
              "   0.25,\n",
              "   0.26666666666666666,\n",
              "   0.21666666666666667,\n",
              "   0.18333333333333332,\n",
              "   0.2909090909090909,\n",
              "   0.32727272727272727,\n",
              "   0.2909090909090909,\n",
              "   0.32727272727272727,\n",
              "   0.2909090909090909,\n",
              "   0.18181818181818182,\n",
              "   0.5357142857142857,\n",
              "   0.5,\n",
              "   0.5357142857142857,\n",
              "   0.5,\n",
              "   0.4642857142857143,\n",
              "   0.32142857142857145,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6086956521739131,\n",
              "   0.43478260869565216,\n",
              "   0.3181818181818182,\n",
              "   0.3409090909090909,\n",
              "   0.3181818181818182,\n",
              "   0.3409090909090909,\n",
              "   0.2954545454545455,\n",
              "   0.22727272727272727]),\n",
              " (3,\n",
              "  [0.11111111111111109,\n",
              "   0.11764705882352941,\n",
              "   0.11111111111111109,\n",
              "   0.11764705882352941,\n",
              "   0.11111111111111109,\n",
              "   0.0718954248366013,\n",
              "   0.25,\n",
              "   0.26666666666666666,\n",
              "   0.25,\n",
              "   0.26666666666666666,\n",
              "   0.21666666666666667,\n",
              "   0.18333333333333332,\n",
              "   0.2909090909090909,\n",
              "   0.32727272727272727,\n",
              "   0.2909090909090909,\n",
              "   0.32727272727272727,\n",
              "   0.2909090909090909,\n",
              "   0.18181818181818182,\n",
              "   0.5357142857142857,\n",
              "   0.5,\n",
              "   0.5357142857142857,\n",
              "   0.5,\n",
              "   0.4642857142857143,\n",
              "   0.32142857142857145,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6086956521739131,\n",
              "   0.43478260869565216,\n",
              "   0.3181818181818182,\n",
              "   0.3409090909090909,\n",
              "   0.3181818181818182,\n",
              "   0.3409090909090909,\n",
              "   0.2954545454545455,\n",
              "   0.22727272727272727]),\n",
              " (4,\n",
              "  [0.11111111111111109,\n",
              "   0.11764705882352941,\n",
              "   0.11111111111111109,\n",
              "   0.11764705882352941,\n",
              "   0.11111111111111109,\n",
              "   0.0718954248366013,\n",
              "   0.25,\n",
              "   0.26666666666666666,\n",
              "   0.25,\n",
              "   0.26666666666666666,\n",
              "   0.21666666666666667,\n",
              "   0.18333333333333332,\n",
              "   0.2909090909090909,\n",
              "   0.32727272727272727,\n",
              "   0.2909090909090909,\n",
              "   0.32727272727272727,\n",
              "   0.2909090909090909,\n",
              "   0.18181818181818182,\n",
              "   0.5357142857142857,\n",
              "   0.5,\n",
              "   0.5357142857142857,\n",
              "   0.5,\n",
              "   0.4642857142857143,\n",
              "   0.32142857142857145,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6086956521739131,\n",
              "   0.43478260869565216,\n",
              "   0.3181818181818182,\n",
              "   0.3409090909090909,\n",
              "   0.3181818181818182,\n",
              "   0.3409090909090909,\n",
              "   0.2954545454545455,\n",
              "   0.22727272727272727]),\n",
              " (5,\n",
              "  [0.11111111111111109,\n",
              "   0.11764705882352941,\n",
              "   0.11111111111111109,\n",
              "   0.11764705882352941,\n",
              "   0.11111111111111109,\n",
              "   0.0718954248366013,\n",
              "   0.25,\n",
              "   0.26666666666666666,\n",
              "   0.25,\n",
              "   0.26666666666666666,\n",
              "   0.21666666666666667,\n",
              "   0.18333333333333332,\n",
              "   0.2909090909090909,\n",
              "   0.32727272727272727,\n",
              "   0.2909090909090909,\n",
              "   0.32727272727272727,\n",
              "   0.2909090909090909,\n",
              "   0.18181818181818182,\n",
              "   0.5357142857142857,\n",
              "   0.5,\n",
              "   0.5357142857142857,\n",
              "   0.5,\n",
              "   0.4642857142857143,\n",
              "   0.32142857142857145,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6521739130434783,\n",
              "   0.6086956521739131,\n",
              "   0.43478260869565216,\n",
              "   0.3181818181818182,\n",
              "   0.3409090909090909,\n",
              "   0.3181818181818182,\n",
              "   0.3409090909090909,\n",
              "   0.2954545454545455,\n",
              "   0.22727272727272727])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guQZ7FWmDkOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvLS20R7Dkb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz7YApkMDkT2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_4BKpKzWLCV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hypotheses.append(predicted1)\n",
        "references.append(cleaned_captions[image1_id])\n",
        "# references.append(cleaned_captions[image1_id + 1])\n",
        "# references.append(cleaned_captions[image1_id + 2])\n",
        "# references.append(cleaned_captions[image1_id + 3])\n",
        "# references.append(cleaned_captions[image1_id + 4])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}