{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Image Caption Generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HurleyJames/NNs/blob/master/Image_Caption_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a31sFIyrHaXl",
        "colab_type": "text"
      },
      "source": [
        "# Image Caption Generation\n",
        "\n",
        "Starter code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81kdnnwJvTFx",
        "colab_type": "text"
      },
      "source": [
        "## Text preparation \n",
        "\n",
        "We need to build a vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-C_TDN97yX_",
        "colab_type": "code",
        "outputId": "289e7f78-baa5-415d-90c7-e3865999a6c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXpWOFqFOXcc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Mounted Drive if using Colab; otherwise, your local path\n",
        "root = \"drive/My Drive/Flickr8k/\" # <--- replace this with your root data directory\n",
        "caption_dir = root + \"captions/\"                       # <--- replace these too\n",
        "image_dir = root + \"images/\"                           # <---\n",
        "\n",
        "\n",
        "token_file = \"Flickr8k.token.txt\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9AkORttFoF_",
        "colab_type": "text"
      },
      "source": [
        "A helper function to read in our ground truth text file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHC0y7zaOXq8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def read_lines(filepath):\n",
        "    \"\"\" Open the ground truth captions into memory, line by line. \"\"\"\n",
        "    file = open(filepath, 'r')\n",
        "    lines = []\n",
        "\n",
        "    while True: \n",
        "        # Get next line from file until there's no more\n",
        "        line = file.readline() \n",
        "        if not line: \n",
        "            break\n",
        "        lines.append(line.strip())\n",
        "    file.close() \n",
        "    return lines"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D86cJx2yv81K",
        "colab_type": "text"
      },
      "source": [
        "You can read all the ground truth captions (5 per image), into memory as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m-snsM2XHuu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lines = read_lines(caption_dir + token_file)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-IkK91ZuXNB2",
        "colab_type": "code",
        "outputId": "2e3e4b66-c683-4144-a728-7a095f23744b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "lines[:5]"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['1000268201_693b08cb0e.jpg#0\\tA child in a pink dress is climbing up a set of stairs in an entry way .',\n",
              " '1000268201_693b08cb0e.jpg#1\\tA girl going into a wooden building .',\n",
              " '1000268201_693b08cb0e.jpg#2\\tA little girl climbing into a wooden playhouse .',\n",
              " '1000268201_693b08cb0e.jpg#3\\tA little girl climbing the stairs to her playhouse .',\n",
              " '1000268201_693b08cb0e.jpg#4\\tA little girl in a pink dress going into a wooden cabin .']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2c_6V2LMUb1q",
        "colab_type": "code",
        "outputId": "16f2ef98-c27d-458e-ebfb-e61a85ee0667",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(lines)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40455"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oksUJjLPwApA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Vocabulary(object):\n",
        "    \"\"\"Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n",
        "    def __init__(self):\n",
        "        # Intially, set both the IDs and words to empty dictionaries.\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.idx = 0\n",
        "\n",
        "    def add_word(self, word):\n",
        "        # If the word does not already exist in the dictionary, add it\n",
        "        if not word in self.word2idx:\n",
        "            self.word2idx[word] = self.idx\n",
        "            self.idx2word[self.idx] = word\n",
        "            # Increment the ID for the next word\n",
        "            self.idx += 1\n",
        "\n",
        "    def __call__(self, word):\n",
        "        # If we try to access a word in the dictionary which does not exist, return the <unk> id\n",
        "        if not word in self.word2idx:\n",
        "            return self.word2idx['<unk>']\n",
        "        return self.word2idx[word]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.word2idx)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VEQtthpXwEoY",
        "colab_type": "text"
      },
      "source": [
        "Extract all the words from ```lines```, and create a list of them in a variable ```words```, for example:\n",
        "\n",
        "```words = [\"a\", \"an\", \"the\", \"cat\"... ]```\n",
        "\n",
        "No need to worry about duplicates.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3IsstzDhbvdb",
        "colab_type": "text"
      },
      "source": [
        "splitting the image ID from the caption text and split the caption text into words and trim any trailing whitespaces"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9M3UWSAwAsM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "image_ids = []\n",
        "captions = []\n",
        "\n",
        "for i in lines:\n",
        "    image_ids.append(i.split('\\t')[0][:-6])\n",
        "    i = i.split('\\t')[1].replace(\" .\", \".\").lower()\n",
        "    i = re.sub(\"[{}]+\".format(string.punctuation), \"\", i)\n",
        "    captions.append(i.split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8K3PtGlCJ4e",
        "colab_type": "code",
        "outputId": "5ed0b6d1-ada9-45c7-d783-8da4eb59117e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        }
      },
      "source": [
        "captions[0]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'child',\n",
              " 'in',\n",
              " 'a',\n",
              " 'pink',\n",
              " 'dress',\n",
              " 'is',\n",
              " 'climbing',\n",
              " 'up',\n",
              " 'a',\n",
              " 'set',\n",
              " 'of',\n",
              " 'stairs',\n",
              " 'in',\n",
              " 'an',\n",
              " 'entry',\n",
              " 'way']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "h32EvDjxngj1",
        "colab": {}
      },
      "source": [
        "total = sum(captions, [])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItTv04EMndZJ",
        "colab_type": "code",
        "outputId": "2408a75d-3262-49ba-ff34-dfea255c52de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "total[:5]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a', 'child', 'in', 'a', 'pink']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XSE8Awm6-S9W",
        "colab_type": "code",
        "outputId": "ec8f7396-9e6a-49dc-b14b-9ce5ce194879",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(total)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "436505"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJF1JovK5KCV",
        "colab_type": "code",
        "outputId": "c7018343-2ab9-489a-fe62-1f50d6f8d418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "word = []\n",
        "# c = Counter((map(tuple, captions)))\n",
        "c = Counter(total)\n",
        "# print(len(list(c.keys())))\n",
        "# print(len(list(c.values())))\n",
        "# print(len(c))\n",
        "print(list(c.keys())[1])\n",
        "for j in range(len(c)):\n",
        "    if (list(c.values())[j] > 3):\n",
        "        word.append(list(c.keys())[j])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "child\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-dT8Q-8O-4aY",
        "colab_type": "code",
        "outputId": "ae5a97a3-908d-4581-bfcd-18335c4561b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(word)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3436"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHBMe-ATwLIQ",
        "colab_type": "text"
      },
      "source": [
        "Build the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ctwErx_ZwAzB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create a vocab instance\n",
        "vocab = Vocabulary()\n",
        "\n",
        "# Add the token words first\n",
        "vocab.add_word('<pad>')\n",
        "vocab.add_word('<start>')\n",
        "vocab.add_word('<end>')\n",
        "vocab.add_word('<unk>')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzEYIvJ-GA_G",
        "colab_type": "text"
      },
      "source": [
        "Add the rest of the words from the parsed captions:\n",
        "\n",
        "``` vocab.add_word('new_word')```\n",
        "\n",
        "Don't add words that appear three times or less."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zj99JT2XwA4-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in word:\n",
        "    vocab.add_word(i)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChA2jcHxX0Eg",
        "colab_type": "code",
        "outputId": "c5422e0f-8444-4f23-f3dd-4216adf7f405",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "len(vocab)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3440"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB30f4wYwSvg",
        "colab_type": "text"
      },
      "source": [
        "## Dataset and loaders for training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raEOHrpnbbKY",
        "colab_type": "text"
      },
      "source": [
        "Keeping the same order, concatenate all the cleaned words from each caption into a string again, and add them all to a list of strings ```cleaned_captions```. Store all the image ids in a list ```image_ids```."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eKax9UJ1xPq",
        "colab_type": "code",
        "outputId": "86d2fdda-af6d-462d-e7a4-d4e188384356",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "image_ids[29706]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'3497238310_2abde3965d'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGGnaDIRbZUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cleaned_captions = []\n",
        "\n",
        "for i in range(len(captions)):\n",
        "    cleaned_captions.append(\" \".join(captions[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_FbII1VwVSg",
        "colab_type": "text"
      },
      "source": [
        "The dataframe for the image paths and captions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYQz4T3mwA2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'image_id': image_ids,\n",
        "    'path': [image_dir + image_id + \".jpg\" for image_id in image_ids],\n",
        "    'caption': cleaned_captions\n",
        "}\n",
        "\n",
        "data_df = pd.DataFrame(data, columns=['image_id', 'path', 'caption'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POB7UiJLwYsf",
        "colab_type": "code",
        "outputId": "0b11ce38-123b-467d-d05e-c6ced435f0ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "data_df.head(n=5)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image_id</th>\n",
              "      <th>path</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1000268201_693b08cb0e</td>\n",
              "      <td>drive/My Drive/Flickr8k/images/1000268201_693b...</td>\n",
              "      <td>a child in a pink dress is climbing up a set o...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1000268201_693b08cb0e</td>\n",
              "      <td>drive/My Drive/Flickr8k/images/1000268201_693b...</td>\n",
              "      <td>a girl going into a wooden building</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1000268201_693b08cb0e</td>\n",
              "      <td>drive/My Drive/Flickr8k/images/1000268201_693b...</td>\n",
              "      <td>a little girl climbing into a wooden playhouse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1000268201_693b08cb0e</td>\n",
              "      <td>drive/My Drive/Flickr8k/images/1000268201_693b...</td>\n",
              "      <td>a little girl climbing the stairs to her playh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1000268201_693b08cb0e</td>\n",
              "      <td>drive/My Drive/Flickr8k/images/1000268201_693b...</td>\n",
              "      <td>a little girl in a pink dress going into a woo...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                image_id  ...                                            caption\n",
              "0  1000268201_693b08cb0e  ...  a child in a pink dress is climbing up a set o...\n",
              "1  1000268201_693b08cb0e  ...                a girl going into a wooden building\n",
              "2  1000268201_693b08cb0e  ...     a little girl climbing into a wooden playhouse\n",
              "3  1000268201_693b08cb0e  ...  a little girl climbing the stairs to her playh...\n",
              "4  1000268201_693b08cb0e  ...  a little girl in a pink dress going into a woo...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyF38cvvgA16",
        "colab_type": "code",
        "outputId": "30fe6493-f71c-4333-861e-c4e669df94a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cleaned_captions[1]"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a girl going into a wooden building'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zNLQ0K-_weJy",
        "colab_type": "text"
      },
      "source": [
        "This is the Flickr8k class for the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqf2_F6YwakD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from PIL import Image\n",
        "import cv2\n",
        "from nltk import tokenize\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Flickr8k(Dataset):\n",
        "    \"\"\" Flickr8k custom dataset compatible with torch.utils.data.DataLoader. \"\"\"\n",
        "    \n",
        "    def __init__(self, df, vocab, transform=None):\n",
        "        \"\"\" Set the path for images, captions and vocabulary wrapper.\n",
        "        \n",
        "        Args:\n",
        "            df: df containing image paths and captions.\n",
        "            vocab: vocabulary wrapper.\n",
        "            transform: image transformer.\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \"\"\" Returns one data pair (image and caption). \"\"\"\n",
        "\n",
        "        vocab = self.vocab\n",
        "\n",
        "        caption = self.df['caption'][index]\n",
        "        img_id = self.df['image_id'][index]\n",
        "        path = self.df['path'][index]\n",
        "\n",
        "        image = Image.open(open(path, 'rb'))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert caption (string) to word ids.\n",
        "        tokens = caption.split()\n",
        "        caption = []\n",
        "        # Build the Tensor version of the caption, with token words\n",
        "        caption.append(vocab('<start>'))\n",
        "        caption.extend([vocab(token) for token in tokens])\n",
        "        caption.append(vocab('<end>'))\n",
        "        target = torch.Tensor(caption)\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vkld_4CwkPO",
        "colab_type": "text"
      },
      "source": [
        "We need to overwrite the default PyTorch ```collate_fn()``` because our ground truth captions are sequential data of varying lengths. The default ```collate_fn()``` does not support merging the captions with padding.\n",
        "\n",
        "You can read more about it here: https://pytorch.org/docs/stable/data.html#dataloader-collate-fn. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5YmKr9ewkqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def caption_collate_fn(data):\n",
        "    \"\"\" Creates mini-batch tensors from the list of tuples (image, caption).\n",
        "    Args:\n",
        "        data: list of tuple (image, caption). \n",
        "            - image: torch tensor of shape (3, 256, 256).\n",
        "            - caption: torch tensor of shape (?); variable length.\n",
        "    Returns:\n",
        "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
        "        targets: torch tensor of shape (batch_size, padded_length).\n",
        "        lengths: list; valid length for each padded caption.\n",
        "    \"\"\"\n",
        "    # Sort a data list by caption length from longest to shortest.\n",
        "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
        "    images, captions = zip(*data)\n",
        "\n",
        "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
        "    lengths = [len(cap) for cap in captions]\n",
        "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
        "    for i, cap in enumerate(captions):\n",
        "        end = lengths[i]\n",
        "        targets[i, :end] = cap[:end]        \n",
        "    return images, targets, lengths"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e6VDx2O5FSiM",
        "colab_type": "text"
      },
      "source": [
        "Now we define the data transform."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpRbVk6BFTGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "# Crop size matches the input dimensions expected by the pre-trained ResNet\n",
        "data_transform = transforms.Compose([ \n",
        "    transforms.Resize(224),\n",
        "    transforms.CenterCrop(224),  # Why do we choose 224 x 224?\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),   # Using ImageNet norms\n",
        "                         (0.229, 0.224, 0.225))])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgS9OpZ7FaAj",
        "colab_type": "text"
      },
      "source": [
        "Initialising the datasets. The only twist is that every image has 5 ground truth captions, so each image appears five times in the dataframe. We don't want an image to appear in more than one set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnTvR684GGVV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "unit_size = 5\n",
        "\n",
        "train_split = 0.95 # Defines the ratio of train/test data.\n",
        "\n",
        "# We didn't shuffle the dataframe yet so this works\n",
        "train_size = unit_size * round(len(data_df)*train_split / unit_size)\n",
        "\n",
        "dataset_train = Flickr8k(\n",
        "    df=data_df[:train_size].reset_index(drop=True),\n",
        "    vocab=vocab,\n",
        "    transform=data_transform,\n",
        ")\n",
        "\n",
        "dataset_test = Flickr8k(\n",
        "    df=data_df[(train_size):].reset_index(drop=True),\n",
        "    vocab=vocab,\n",
        "    transform=data_transform,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWuWg72dGOq9",
        "colab_type": "text"
      },
      "source": [
        "Write the dataloaders ```train_loader``` and ```test_loader``` - explicitly replacing the collate_fn:\n",
        "\n",
        "```train_loader = torch.utils.data.DataLoader(\n",
        "  ...,\n",
        "  collate_fn=caption_collate_fn\n",
        ")```\n",
        "\n",
        "Set train batch size to 128 and be sure to set ```shuffle=True```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkNrIRbXGLFG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(\n",
        "    dataset_train,\n",
        "    batch_size=128,\n",
        "    shuffle=True,\n",
        "    collate_fn=caption_collate_fn\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    dataset_test,\n",
        "    batch_size=128,\n",
        "    shuffle=False,\n",
        "    collate_fn=caption_collate_fn\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXlf8lt5TF0N",
        "colab_type": "text"
      },
      "source": [
        "## Encoder and decoder models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ls8lyXA2GTC0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
        "        super(EncoderCNN, self).__init__()\n",
        "        resnet = models.resnet152(pretrained=True) # Pre-trained on ImageNet by default\n",
        "        layers = list(resnet.children())[:-1]      # Keep all layers except the last one\n",
        "        # Unpack the layers and create a new Sequential\n",
        "        self.resnet = nn.Sequential(*layers)\n",
        "        \n",
        "        # We want a specific output size, which is the size of our embedding, so\n",
        "        # we feed our extracted features from the last fc layer (dimensions 1 x 1000)\n",
        "        # into a Linear layer to resize\n",
        "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
        "        \n",
        "        # Batch normalisation helps to speed up training\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "        \n",
        "    def forward(self, images):\n",
        "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
        "        \n",
        "        # Complete graph here. Remember to put the ResNet layer in a with torch.no_grad() block\n",
        "        with torch.no_grad():\n",
        "            features = self.resnet(images)\n",
        "        # print(\"resnet_out:\", features.shape)\n",
        "        features = features.view(features.size(0), -1)\n",
        "        # features = features.reshape(features.size(0), -1)\n",
        "        # print(\"reshape:\", features.shape)\n",
        "        features = self.linear(features)\n",
        "        # print(\"linear:\", features.shape)\n",
        "        features = self.bn(features)\n",
        "\n",
        "        return features\n",
        "\n",
        "\n",
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
        "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        \n",
        "        # What is an embedding layer?\n",
        "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "        # Define this layer (one at a time)\n",
        "        # self.lstm / self.rnn\n",
        "        self.lstm = nn.RNN(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        # self.lstm = nn.LSTM(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
        "        \n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.max_seq_length = max_seq_length\n",
        "        \n",
        "    def forward(self, features, captions, lengths):\n",
        "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
        "        embeddings = self.embed(captions)\n",
        "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
        "        # What is \"packing\" a padded sequence?\n",
        "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
        "\n",
        "        hiddens, _ = self.lstm(packed) # Replace with self.rnn when using RNN\n",
        "        outputs = self.linear(hiddens[0])\n",
        "        return outputs\n",
        "    \n",
        "    def sample(self, features, states=None):\n",
        "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
        "        sampled_ids = []\n",
        "        inputs = features.unsqueeze(1)\n",
        "        for i in range(self.max_seq_length):\n",
        "            hiddens, states = self.lstm(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
        "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
        "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
        "            sampled_ids.append(predicted)\n",
        "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
        "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
        "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
        "        return sampled_ids"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9-qtkkMTrtB",
        "colab_type": "code",
        "outputId": "f2bfd8ea-9ef2-46e1-8358-851ef800666c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhecFOMRUgpe",
        "colab_type": "text"
      },
      "source": [
        "Set training parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Fd2-IX2Uer3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed_size = 256\n",
        "hidden_size = 512\n",
        "num_layers = 1\n",
        "learning_rate = 0.001\n",
        "# num_epochs = 1\n",
        "num_epochs = 5\n",
        "log_step = 10\n",
        "save_step = 1\n",
        "\n",
        "model_path= root + \"models/\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlIwF6P8UgB4",
        "colab_type": "text"
      },
      "source": [
        "Initialize the models and set the learning parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxwDUlR2Uy7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Build the models\n",
        "encoder = EncoderCNN(embed_size).to(device)\n",
        "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Optimisation will be on the parameters of BOTH the enocder and decoder,\n",
        "# but excluding the ResNet parameters, only the new added layers.\n",
        "params = list(\n",
        "    decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters()\n",
        ")\n",
        "\n",
        "optimizer = torch.optim.Adam(params, lr=learning_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RUmSb2MHEZw3",
        "colab_type": "text"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uS4oN21vNKu7",
        "colab_type": "text"
      },
      "source": [
        "The loop to train the model. Feel free to put this in a function if you prefer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5M7KY9G3NI8l",
        "colab_type": "code",
        "outputId": "962af314-76af-45d4-8eea-1786be67a7a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import os\n",
        "\n",
        "encoder.train()\n",
        "decoder.train()\n",
        "\n",
        "if not os.path.exists(model_path):\n",
        "    os.makedirs(model_path)\n",
        "\n",
        "# before training\n",
        "torch.save(encoder.state_dict(), os.path.join(model_path, 'RNN-encoder-0.ckpt'))\n",
        "torch.save(decoder.state_dict(), os.path.join(model_path, 'RNN-decoder-0.ckpt'))\n",
        "\n",
        "# Train the models\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, captions, lengths) in enumerate(train_loader):\n",
        "\n",
        "        # Set mini-batch dataset\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "\n",
        "        # Packed as well as we'll compare to the decoder outputs\n",
        "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
        "\n",
        "        # Forward, backward and optimize\n",
        "        features = encoder(images)\n",
        "        outputs = decoder(features, captions, lengths)\n",
        "\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        # Zero gradients for both networks\n",
        "        decoder.zero_grad()\n",
        "        encoder.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # Print log info\n",
        "        if i % log_step == 0:\n",
        "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
        "                  .format(epoch, num_epochs, i, total_step, loss.item())) \n",
        "\n",
        "        # If you want to save the model checkpoints - recommended once you have everything working\n",
        "        # Make sure to save RNN and LSTM versions separately\n",
        "        # if (i+1) % save_step == 0:\n",
        "        #     torch.save(decoder.state_dict(), os.path.join(model_path, 'decoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
        "        #     torch.save(encoder.state_dict(), os.path.join(model_path, 'encoder-{}-{}.ckpt'.format(epoch+1, i+1)))\n",
        "    torch.save(decoder.state_dict(), os.path.join(model_path, 'RNN-decoder-{}.ckpt'.format(epoch+1)))\n",
        "    torch.save(encoder.state_dict(), os.path.join(model_path, 'RNN-encoder-{}.ckpt'.format(epoch+1)))\n",
        "print('Finished')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [0/5], Step [0/301], Loss: 8.1846\n",
            "Epoch [0/5], Step [10/301], Loss: 5.3010\n",
            "Epoch [0/5], Step [20/301], Loss: 4.9971\n",
            "Epoch [0/5], Step [30/301], Loss: 4.6996\n",
            "Epoch [0/5], Step [40/301], Loss: 4.4780\n",
            "Epoch [0/5], Step [50/301], Loss: 4.1019\n",
            "Epoch [0/5], Step [60/301], Loss: 3.8446\n",
            "Epoch [0/5], Step [70/301], Loss: 3.6938\n",
            "Epoch [0/5], Step [80/301], Loss: 3.5975\n",
            "Epoch [0/5], Step [90/301], Loss: 3.7773\n",
            "Epoch [0/5], Step [100/301], Loss: 3.7025\n",
            "Epoch [0/5], Step [110/301], Loss: 3.5169\n",
            "Epoch [0/5], Step [120/301], Loss: 3.5690\n",
            "Epoch [0/5], Step [130/301], Loss: 3.4571\n",
            "Epoch [0/5], Step [140/301], Loss: 3.3618\n",
            "Epoch [0/5], Step [150/301], Loss: 3.3408\n",
            "Epoch [0/5], Step [160/301], Loss: 3.3129\n",
            "Epoch [0/5], Step [170/301], Loss: 3.3396\n",
            "Epoch [0/5], Step [180/301], Loss: 3.3181\n",
            "Epoch [0/5], Step [190/301], Loss: 3.3559\n",
            "Epoch [0/5], Step [200/301], Loss: 3.3897\n",
            "Epoch [0/5], Step [210/301], Loss: 3.1417\n",
            "Epoch [0/5], Step [220/301], Loss: 3.1890\n",
            "Epoch [0/5], Step [230/301], Loss: 3.1578\n",
            "Epoch [0/5], Step [240/301], Loss: 3.1571\n",
            "Epoch [0/5], Step [250/301], Loss: 3.0674\n",
            "Epoch [0/5], Step [260/301], Loss: 3.1513\n",
            "Epoch [0/5], Step [270/301], Loss: 2.9909\n",
            "Epoch [0/5], Step [280/301], Loss: 3.0830\n",
            "Epoch [0/5], Step [290/301], Loss: 3.0092\n",
            "Epoch [0/5], Step [300/301], Loss: 2.7934\n",
            "Epoch [1/5], Step [0/301], Loss: 3.0098\n",
            "Epoch [1/5], Step [10/301], Loss: 3.0731\n",
            "Epoch [1/5], Step [20/301], Loss: 2.9554\n",
            "Epoch [1/5], Step [30/301], Loss: 3.0079\n",
            "Epoch [1/5], Step [40/301], Loss: 3.0055\n",
            "Epoch [1/5], Step [50/301], Loss: 2.9929\n",
            "Epoch [1/5], Step [60/301], Loss: 2.9271\n",
            "Epoch [1/5], Step [70/301], Loss: 3.0337\n",
            "Epoch [1/5], Step [80/301], Loss: 3.0476\n",
            "Epoch [1/5], Step [90/301], Loss: 2.8926\n",
            "Epoch [1/5], Step [100/301], Loss: 2.9551\n",
            "Epoch [1/5], Step [110/301], Loss: 2.8230\n",
            "Epoch [1/5], Step [120/301], Loss: 2.9971\n",
            "Epoch [1/5], Step [130/301], Loss: 2.8241\n",
            "Epoch [1/5], Step [140/301], Loss: 2.7850\n",
            "Epoch [1/5], Step [150/301], Loss: 2.8692\n",
            "Epoch [1/5], Step [160/301], Loss: 2.8545\n",
            "Epoch [1/5], Step [170/301], Loss: 2.9588\n",
            "Epoch [1/5], Step [180/301], Loss: 2.8621\n",
            "Epoch [1/5], Step [190/301], Loss: 2.9946\n",
            "Epoch [1/5], Step [200/301], Loss: 2.7765\n",
            "Epoch [1/5], Step [210/301], Loss: 2.8001\n",
            "Epoch [1/5], Step [220/301], Loss: 2.7839\n",
            "Epoch [1/5], Step [230/301], Loss: 2.8080\n",
            "Epoch [1/5], Step [240/301], Loss: 2.9275\n",
            "Epoch [1/5], Step [250/301], Loss: 2.7406\n",
            "Epoch [1/5], Step [260/301], Loss: 2.8592\n",
            "Epoch [1/5], Step [270/301], Loss: 2.8210\n",
            "Epoch [1/5], Step [280/301], Loss: 2.8622\n",
            "Epoch [1/5], Step [290/301], Loss: 2.7130\n",
            "Epoch [1/5], Step [300/301], Loss: 2.5836\n",
            "Epoch [2/5], Step [0/301], Loss: 2.7652\n",
            "Epoch [2/5], Step [10/301], Loss: 2.7203\n",
            "Epoch [2/5], Step [20/301], Loss: 2.6697\n",
            "Epoch [2/5], Step [30/301], Loss: 2.5397\n",
            "Epoch [2/5], Step [40/301], Loss: 2.5354\n",
            "Epoch [2/5], Step [50/301], Loss: 2.6705\n",
            "Epoch [2/5], Step [60/301], Loss: 2.7404\n",
            "Epoch [2/5], Step [70/301], Loss: 2.7156\n",
            "Epoch [2/5], Step [80/301], Loss: 2.5501\n",
            "Epoch [2/5], Step [90/301], Loss: 2.6734\n",
            "Epoch [2/5], Step [100/301], Loss: 2.5411\n",
            "Epoch [2/5], Step [110/301], Loss: 2.6314\n",
            "Epoch [2/5], Step [120/301], Loss: 2.5747\n",
            "Epoch [2/5], Step [130/301], Loss: 2.5951\n",
            "Epoch [2/5], Step [140/301], Loss: 2.6171\n",
            "Epoch [2/5], Step [150/301], Loss: 2.6086\n",
            "Epoch [2/5], Step [160/301], Loss: 2.6953\n",
            "Epoch [2/5], Step [170/301], Loss: 2.7207\n",
            "Epoch [2/5], Step [180/301], Loss: 2.7398\n",
            "Epoch [2/5], Step [190/301], Loss: 2.5975\n",
            "Epoch [2/5], Step [200/301], Loss: 2.5846\n",
            "Epoch [2/5], Step [210/301], Loss: 2.5709\n",
            "Epoch [2/5], Step [220/301], Loss: 2.5878\n",
            "Epoch [2/5], Step [230/301], Loss: 2.5156\n",
            "Epoch [2/5], Step [240/301], Loss: 2.6663\n",
            "Epoch [2/5], Step [250/301], Loss: 2.6691\n",
            "Epoch [2/5], Step [260/301], Loss: 2.4997\n",
            "Epoch [2/5], Step [270/301], Loss: 2.6539\n",
            "Epoch [2/5], Step [280/301], Loss: 2.6640\n",
            "Epoch [2/5], Step [290/301], Loss: 2.6644\n",
            "Epoch [2/5], Step [300/301], Loss: 2.5358\n",
            "Epoch [3/5], Step [0/301], Loss: 2.4993\n",
            "Epoch [3/5], Step [10/301], Loss: 2.4783\n",
            "Epoch [3/5], Step [20/301], Loss: 2.4740\n",
            "Epoch [3/5], Step [30/301], Loss: 2.5109\n",
            "Epoch [3/5], Step [40/301], Loss: 2.5126\n",
            "Epoch [3/5], Step [50/301], Loss: 2.5114\n",
            "Epoch [3/5], Step [60/301], Loss: 2.4241\n",
            "Epoch [3/5], Step [70/301], Loss: 2.4872\n",
            "Epoch [3/5], Step [80/301], Loss: 2.5760\n",
            "Epoch [3/5], Step [90/301], Loss: 2.5081\n",
            "Epoch [3/5], Step [100/301], Loss: 2.4143\n",
            "Epoch [3/5], Step [110/301], Loss: 2.4354\n",
            "Epoch [3/5], Step [120/301], Loss: 2.4894\n",
            "Epoch [3/5], Step [130/301], Loss: 2.4553\n",
            "Epoch [3/5], Step [140/301], Loss: 2.3210\n",
            "Epoch [3/5], Step [150/301], Loss: 2.4878\n",
            "Epoch [3/5], Step [160/301], Loss: 2.4178\n",
            "Epoch [3/5], Step [170/301], Loss: 2.5283\n",
            "Epoch [3/5], Step [180/301], Loss: 2.5269\n",
            "Epoch [3/5], Step [190/301], Loss: 2.5217\n",
            "Epoch [3/5], Step [200/301], Loss: 2.4945\n",
            "Epoch [3/5], Step [210/301], Loss: 2.4897\n",
            "Epoch [3/5], Step [220/301], Loss: 2.5314\n",
            "Epoch [3/5], Step [230/301], Loss: 2.6624\n",
            "Epoch [3/5], Step [240/301], Loss: 2.5284\n",
            "Epoch [3/5], Step [250/301], Loss: 2.4167\n",
            "Epoch [3/5], Step [260/301], Loss: 2.3739\n",
            "Epoch [3/5], Step [270/301], Loss: 2.4642\n",
            "Epoch [3/5], Step [280/301], Loss: 2.4154\n",
            "Epoch [3/5], Step [290/301], Loss: 2.3973\n",
            "Epoch [3/5], Step [300/301], Loss: 2.3761\n",
            "Epoch [4/5], Step [0/301], Loss: 2.3688\n",
            "Epoch [4/5], Step [10/301], Loss: 2.2732\n",
            "Epoch [4/5], Step [20/301], Loss: 2.3505\n",
            "Epoch [4/5], Step [30/301], Loss: 2.4048\n",
            "Epoch [4/5], Step [40/301], Loss: 2.3638\n",
            "Epoch [4/5], Step [50/301], Loss: 2.2394\n",
            "Epoch [4/5], Step [60/301], Loss: 2.3701\n",
            "Epoch [4/5], Step [70/301], Loss: 2.1946\n",
            "Epoch [4/5], Step [80/301], Loss: 2.2913\n",
            "Epoch [4/5], Step [90/301], Loss: 2.2699\n",
            "Epoch [4/5], Step [100/301], Loss: 2.3012\n",
            "Epoch [4/5], Step [110/301], Loss: 2.2203\n",
            "Epoch [4/5], Step [120/301], Loss: 2.3284\n",
            "Epoch [4/5], Step [130/301], Loss: 2.2214\n",
            "Epoch [4/5], Step [140/301], Loss: 2.4100\n",
            "Epoch [4/5], Step [150/301], Loss: 2.2772\n",
            "Epoch [4/5], Step [160/301], Loss: 2.3533\n",
            "Epoch [4/5], Step [170/301], Loss: 2.4560\n",
            "Epoch [4/5], Step [180/301], Loss: 2.3839\n",
            "Epoch [4/5], Step [190/301], Loss: 2.3366\n",
            "Epoch [4/5], Step [200/301], Loss: 2.3337\n",
            "Epoch [4/5], Step [210/301], Loss: 2.3397\n",
            "Epoch [4/5], Step [220/301], Loss: 2.3412\n",
            "Epoch [4/5], Step [230/301], Loss: 2.3186\n",
            "Epoch [4/5], Step [240/301], Loss: 2.2834\n",
            "Epoch [4/5], Step [250/301], Loss: 2.3951\n",
            "Epoch [4/5], Step [260/301], Loss: 2.3142\n",
            "Epoch [4/5], Step [270/301], Loss: 2.3578\n",
            "Epoch [4/5], Step [280/301], Loss: 2.3925\n",
            "Epoch [4/5], Step [290/301], Loss: 2.4069\n",
            "Epoch [4/5], Step [300/301], Loss: 2.2928\n",
            "Finished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7HLBDrWJp8n",
        "colab_type": "text"
      },
      "source": [
        "## Random Interface from dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hf2_gRdOJup4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "def getRandomImage(img_list):\n",
        "    \"\"\"Returns a random filename, choose among the files of the given path\"\"\"\n",
        "    index = random.randrange(0, len(img_list))\n",
        "    return img_list[index], index\n",
        "\n",
        "def loadImage(image_path, transform=None):\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize([224, 224])\n",
        "\n",
        "    if transform is not None:\n",
        "        image = transform(image).unsqueeze(0)\n",
        "\n",
        "    return image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nza_uE7PWnYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_list = []\n",
        "img_path = caption_dir + \"Flickr_8k.testImages.txt\"\n",
        "\n",
        "with open(img_path, 'r') as f:\n",
        "    for i in f:\n",
        "        img_list.append(i.strip())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PFGsg30nAfBU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# choose two random images for evaluation\n",
        "filename1, index1 = getRandomImage(img_list)\n",
        "filename2, index2 = getRandomImage(img_list)\n",
        "image1 = loadImage(image_dir + str(filename1), data_transform)\n",
        "image2 = loadImage(image_dir + str(filename2), data_transform) \n",
        "\n",
        "# image = Image.open(image_dir + str(filename1))\n",
        "# plt.imshow(np.asarray(image1))\n",
        "# plt.imshow(np.asarray(image2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "onpfSoEZ-lRR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "image1_id = image_ids.index(filename1[:-4])\n",
        "image2_id = image_ids.index(filename2[:-4])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUQ0mOQ5Mluu",
        "colab_type": "text"
      },
      "source": [
        "Loading checkpoint and initializing encoder and decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swQMgv1HMsQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch_checkpoint = ['0', '1', '2', '3', '4', '5']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6D-N3iX21j3n",
        "colab_type": "text"
      },
      "source": [
        "## BLEU for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFUJ3Bg62TOb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "\n",
        "def transform_idx_to_words(input):\n",
        "    sampled_caption = []\n",
        "\n",
        "    for idx in input:\n",
        "        word = vocab.idx2word[idx]\n",
        "        sampled_caption.append(word)\n",
        "\n",
        "        if word == '<end>':\n",
        "            break\n",
        "    \n",
        "    output = ' '.join(sampled_caption[1:-1])\n",
        "    output = output.replace(' ,', ',')\n",
        "    \n",
        "    return output.split(' ')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XucCmq4AXvH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "f9f7a966-86ef-42e6-a686-90de6ad4ea15"
      },
      "source": [
        "bleu_score1 = []\n",
        "bleu_score2 = []\n",
        "# an image has 5 reference captions\n",
        "bleu_reference1 = []\n",
        "bleu_reference2 = []\n",
        "\n",
        "for model_name in range(len(epoch_checkpoint)):\n",
        "    print(model_name)\n",
        "    encoder_model_path = os.path.join(model_path, 'RNN-encoder-{}.ckpt'.format(model_name))\n",
        "    decoder_model_path = os.path.join(model_path, 'RNN-decoder-{}.ckpt'.format(model_name))\n",
        "\n",
        "    test_encoder = EncoderCNN(embed_size)\n",
        "    test_decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers)    \n",
        "\n",
        "    test_encoder.load_state_dict(torch.load(encoder_model_path))\n",
        "    test_decoder.load_state_dict(torch.load(decoder_model_path))\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        test_encoder = test_encoder.to(device)\n",
        "        test_decoder = test_decoder.to(device)\n",
        "\n",
        "    test_encoder.eval()\n",
        "    test_decoder.eval()\n",
        "\n",
        "    image1 = image1.to(device)\n",
        "    image2 = image2.to(device)\n",
        "\n",
        "    predicted, actual = list(), list()\n",
        "\n",
        "    feature1 = test_encoder(image1)\n",
        "    cap1 = test_decoder.sample(feature1)\n",
        "    cap1 = cap1.cpu().data.numpy()\n",
        "    cap1 = cap1[0,:]\n",
        "\n",
        "    feature2 = test_encoder(image2)\n",
        "    cap2 = test_decoder.sample(feature2)\n",
        "    cap2 = cap2.cpu().data.numpy()\n",
        "    cap2 = cap2[0,:]\n",
        "\n",
        "    predicted.append(cap1)\n",
        "    predicted.append(cap2)\n",
        "\n",
        "    predicted1 = \" \".join(transform_idx_to_words(predicted[0]))\n",
        "    predicted2 = \" \".join(transform_idx_to_words(predicted[1]))\n",
        "\n",
        "    print(\"The information of 1st image:\" + filename1 + \"     \" + str(image1_id))\n",
        "    print(\"The generated caption of 1st image: \" + predicted1)\n",
        "\n",
        "    hypotheses = list()\n",
        "    references = list()\n",
        "\n",
        "    # 1st image\n",
        "    hypotheses.append(predicted1)\n",
        "    for i in range(len(epoch_checkpoint)):\n",
        "        references.append(cleaned_captions[image1_id + i])\n",
        "        print('The {} reference caption of 1st image: '.format(i) + references[0])\n",
        "        bleu = corpus_bleu(references, hypotheses, weights=(1.0/1.0,))\n",
        "        # TODO\n",
        "        # bleu4 = \n",
        "        print('BLEU score with {} reference caption: '.format(i) + str(bleu))\n",
        "        bleu_reference1.append(bleu)\n",
        "\n",
        "        references.clear();\n",
        "\n",
        "    # 2nd image\n",
        "    print(\"The information of 2nd image:\" + filename2 + \"     \" + str(image2_id))\n",
        "    print(\"The generated caption of 2nd image: \" + predicted2)\n",
        "\n",
        "    hypotheses.clear()\n",
        "    references.clear()\n",
        "\n",
        "    hypotheses.append(predicted2)\n",
        "    for i in range(len(epoch_checkpoint)):\n",
        "        references.append(cleaned_captions[image2_id + i])\n",
        "        print('The {} reference caption of 2nd image: '.format(i) + references[0])\n",
        "        bleu = corpus_bleu(references, hypotheses, weights=(1.0/1.0,))\n",
        "        print('BLEU score with {} reference caption: '.format(i) + str(bleu))\n",
        "        bleu_reference2.append(bleu)\n",
        "\n",
        "        references.clear();\n",
        "\n",
        "    bleu_score1.append((model_name, bleu_reference1))\n",
        "    bleu_score2.append((model_name, bleu_reference2))\n",
        "    print('done for model: {}'.format(model_name))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "The information of 1st image:226607225_44d696db6b.jpg     6830\n",
            "The generated caption of 1st image: ski conversation milkshake zoo fit miami bagpipe wielding branches observed laying a natural footballers wig ties carries others\n",
            "The 0 reference caption of 1st image: a green fish is jumping out of water\n",
            "BLEU score with 0 reference caption: 0.12500000000000003\n",
            "The 1 reference caption of 1st image: a large fish breaching the water s surface\n",
            "BLEU score with 1 reference caption: 0.12500000000000003\n",
            "The 2 reference caption of 1st image: a large green and blue fish leaps from the water\n",
            "BLEU score with 2 reference caption: 0.1484375\n",
            "The 3 reference caption of 1st image: a large green fish jumps out of the water\n",
            "BLEU score with 3 reference caption: 0.1328125\n",
            "The 4 reference caption of 1st image: the large green fish is jumping out of the water\n",
            "BLEU score with 4 reference caption: 0.1328125\n",
            "The 5 reference caption of 1st image: a man in a black tshirt looking down as he holds a drink\n",
            "BLEU score with 5 reference caption: 0.140625\n",
            "The information of 2nd image:3107513635_fe8a21f148.jpg     20285\n",
            "The generated caption of 2nd image: glider wig rusty dancers cans arm walker stomach cabin cigar professional park movie striped meeting hockey asian jogger\n",
            "The 0 reference caption of 2nd image: a bunch of people at a train station\n",
            "BLEU score with 0 reference caption: 0.13333333333333333\n",
            "The 1 reference caption of 2nd image: a photo of a train platform on a chilly day\n",
            "BLEU score with 1 reference caption: 0.12500000000000003\n",
            "The 2 reference caption of 2nd image: a train station says tychy miasto\n",
            "BLEU score with 2 reference caption: 0.10000000000000002\n",
            "The 3 reference caption of 2nd image: people are boarding two yellow and blue trains\n",
            "BLEU score with 3 reference caption: 0.14166666666666666\n",
            "The 4 reference caption of 2nd image: several people waiting outside a train station\n",
            "BLEU score with 4 reference caption: 0.13333333333333333\n",
            "The 5 reference caption of 2nd image: two girls take in the view\n",
            "BLEU score with 5 reference caption: 0.12500000000000003\n",
            "done for model: 0\n",
            "1\n",
            "The information of 1st image:226607225_44d696db6b.jpg     6830\n",
            "The generated caption of 1st image: a boy in a blue shirt is running on a snowy mountain\n",
            "The 0 reference caption of 1st image: a green fish is jumping out of water\n",
            "BLEU score with 0 reference caption: 0.2692307692307693\n",
            "The 1 reference caption of 1st image: a large fish breaching the water s surface\n",
            "BLEU score with 1 reference caption: 0.2692307692307693\n",
            "The 2 reference caption of 1st image: a large green and blue fish leaps from the water\n",
            "BLEU score with 2 reference caption: 0.30769230769230765\n",
            "The 3 reference caption of 1st image: a large green fish jumps out of the water\n",
            "BLEU score with 3 reference caption: 0.28846153846153844\n",
            "The 4 reference caption of 1st image: the large green fish is jumping out of the water\n",
            "BLEU score with 4 reference caption: 0.28846153846153844\n",
            "The 5 reference caption of 1st image: a man in a black tshirt looking down as he holds a drink\n",
            "BLEU score with 5 reference caption: 0.28846153846153844\n",
            "The information of 2nd image:3107513635_fe8a21f148.jpg     20285\n",
            "The generated caption of 2nd image: a group of people are standing in front of a crowd of people\n",
            "The 0 reference caption of 2nd image: a bunch of people at a train station\n",
            "BLEU score with 0 reference caption: 0.23333333333333334\n",
            "The 1 reference caption of 2nd image: a photo of a train platform on a chilly day\n",
            "BLEU score with 1 reference caption: 0.2\n",
            "The 2 reference caption of 2nd image: a train station says tychy miasto\n",
            "BLEU score with 2 reference caption: 0.15\n",
            "The 3 reference caption of 2nd image: people are boarding two yellow and blue trains\n",
            "BLEU score with 3 reference caption: 0.25\n",
            "The 4 reference caption of 2nd image: several people waiting outside a train station\n",
            "BLEU score with 4 reference caption: 0.25\n",
            "The 5 reference caption of 2nd image: two girls take in the view\n",
            "BLEU score with 5 reference caption: 0.2\n",
            "done for model: 1\n",
            "2\n",
            "The information of 1st image:226607225_44d696db6b.jpg     6830\n",
            "The generated caption of 1st image: a surfer is riding a wave\n",
            "The 0 reference caption of 1st image: a green fish is jumping out of water\n",
            "BLEU score with 0 reference caption: 0.44\n",
            "The 1 reference caption of 1st image: a large fish breaching the water s surface\n",
            "BLEU score with 1 reference caption: 0.44\n",
            "The 2 reference caption of 1st image: a large green and blue fish leaps from the water\n",
            "BLEU score with 2 reference caption: 0.48\n",
            "The 3 reference caption of 1st image: a large green fish jumps out of the water\n",
            "BLEU score with 3 reference caption: 0.44\n",
            "The 4 reference caption of 1st image: the large green fish is jumping out of the water\n",
            "BLEU score with 4 reference caption: 0.44\n",
            "The 5 reference caption of 1st image: a man in a black tshirt looking down as he holds a drink\n",
            "BLEU score with 5 reference caption: 0.4\n",
            "The information of 2nd image:3107513635_fe8a21f148.jpg     20285\n",
            "The generated caption of 2nd image: a man in a black shirt and a white shirt and a white shirt and a backpack\n",
            "The 0 reference caption of 2nd image: a bunch of people at a train station\n",
            "BLEU score with 0 reference caption: 0.1780821917808219\n",
            "The 1 reference caption of 2nd image: a photo of a train platform on a chilly day\n",
            "BLEU score with 1 reference caption: 0.1643835616438356\n",
            "The 2 reference caption of 2nd image: a train station says tychy miasto\n",
            "BLEU score with 2 reference caption: 0.136986301369863\n",
            "The 3 reference caption of 2nd image: people are boarding two yellow and blue trains\n",
            "BLEU score with 3 reference caption: 0.1780821917808219\n",
            "The 4 reference caption of 2nd image: several people waiting outside a train station\n",
            "BLEU score with 4 reference caption: 0.1643835616438356\n",
            "The 5 reference caption of 2nd image: two girls take in the view\n",
            "BLEU score with 5 reference caption: 0.1643835616438356\n",
            "done for model: 2\n",
            "3\n",
            "The information of 1st image:226607225_44d696db6b.jpg     6830\n",
            "The generated caption of 1st image: a person in a black wetsuit is surfing in the water\n",
            "The 0 reference caption of 1st image: a green fish is jumping out of water\n",
            "BLEU score with 0 reference caption: 0.29411764705882354\n",
            "The 1 reference caption of 1st image: a large fish breaching the water s surface\n",
            "BLEU score with 1 reference caption: 0.3137254901960784\n",
            "The 2 reference caption of 1st image: a large green and blue fish leaps from the water\n",
            "BLEU score with 2 reference caption: 0.3333333333333333\n",
            "The 3 reference caption of 1st image: a large green fish jumps out of the water\n",
            "BLEU score with 3 reference caption: 0.3137254901960784\n",
            "The 4 reference caption of 1st image: the large green fish is jumping out of the water\n",
            "BLEU score with 4 reference caption: 0.3137254901960784\n",
            "The 5 reference caption of 1st image: a man in a black tshirt looking down as he holds a drink\n",
            "BLEU score with 5 reference caption: 0.3137254901960784\n",
            "The information of 2nd image:3107513635_fe8a21f148.jpg     20285\n",
            "The generated caption of 2nd image: a group of people are standing in front of a building\n",
            "The 0 reference caption of 2nd image: a bunch of people at a train station\n",
            "BLEU score with 0 reference caption: 0.2641509433962264\n",
            "The 1 reference caption of 2nd image: a photo of a train platform on a chilly day\n",
            "BLEU score with 1 reference caption: 0.20754716981132076\n",
            "The 2 reference caption of 2nd image: a train station says tychy miasto\n",
            "BLEU score with 2 reference caption: 0.1509433962264151\n",
            "The 3 reference caption of 2nd image: people are boarding two yellow and blue trains\n",
            "BLEU score with 3 reference caption: 0.28301886792452824\n",
            "The 4 reference caption of 2nd image: several people waiting outside a train station\n",
            "BLEU score with 4 reference caption: 0.2641509433962264\n",
            "The 5 reference caption of 2nd image: two girls take in the view\n",
            "BLEU score with 5 reference caption: 0.20754716981132076\n",
            "done for model: 3\n",
            "4\n",
            "The information of 1st image:226607225_44d696db6b.jpg     6830\n",
            "The generated caption of 1st image: a child in a wetsuit is surfing in the water\n",
            "The 0 reference caption of 1st image: a green fish is jumping out of water\n",
            "BLEU score with 0 reference caption: 0.2954545454545455\n",
            "The 1 reference caption of 1st image: a large fish breaching the water s surface\n",
            "BLEU score with 1 reference caption: 0.3409090909090909\n",
            "The 2 reference caption of 1st image: a large green and blue fish leaps from the water\n",
            "BLEU score with 2 reference caption: 0.3409090909090909\n",
            "The 3 reference caption of 1st image: a large green fish jumps out of the water\n",
            "BLEU score with 3 reference caption: 0.3181818181818182\n",
            "The 4 reference caption of 1st image: the large green fish is jumping out of the water\n",
            "BLEU score with 4 reference caption: 0.3181818181818182\n",
            "The 5 reference caption of 1st image: a man in a black tshirt looking down as he holds a drink\n",
            "BLEU score with 5 reference caption: 0.3181818181818182\n",
            "The information of 2nd image:3107513635_fe8a21f148.jpg     20285\n",
            "The generated caption of 2nd image: a group of people are standing outside of a building\n",
            "The 0 reference caption of 2nd image: a bunch of people at a train station\n",
            "BLEU score with 0 reference caption: 0.2692307692307693\n",
            "The 1 reference caption of 2nd image: a photo of a train platform on a chilly day\n",
            "BLEU score with 1 reference caption: 0.21153846153846154\n",
            "The 2 reference caption of 2nd image: a train station says tychy miasto\n",
            "BLEU score with 2 reference caption: 0.15384615384615385\n",
            "The 3 reference caption of 2nd image: people are boarding two yellow and blue trains\n",
            "BLEU score with 3 reference caption: 0.28846153846153844\n",
            "The 4 reference caption of 2nd image: several people waiting outside a train station\n",
            "BLEU score with 4 reference caption: 0.2692307692307693\n",
            "The 5 reference caption of 2nd image: two girls take in the view\n",
            "BLEU score with 5 reference caption: 0.21153846153846154\n",
            "done for model: 4\n",
            "5\n",
            "The information of 1st image:226607225_44d696db6b.jpg     6830\n",
            "The generated caption of 1st image: a person is swimming underwater in a pool\n",
            "The 0 reference caption of 1st image: a green fish is jumping out of water\n",
            "BLEU score with 0 reference caption: 0.34146341463414637\n",
            "The 1 reference caption of 1st image: a large fish breaching the water s surface\n",
            "BLEU score with 1 reference caption: 0.2926829268292683\n",
            "The 2 reference caption of 1st image: a large green and blue fish leaps from the water\n",
            "BLEU score with 2 reference caption: 0.3902439024390244\n",
            "The 3 reference caption of 1st image: a large green fish jumps out of the water\n",
            "BLEU score with 3 reference caption: 0.3658536585365854\n",
            "The 4 reference caption of 1st image: the large green fish is jumping out of the water\n",
            "BLEU score with 4 reference caption: 0.3658536585365854\n",
            "The 5 reference caption of 1st image: a man in a black tshirt looking down as he holds a drink\n",
            "BLEU score with 5 reference caption: 0.34146341463414637\n",
            "The information of 2nd image:3107513635_fe8a21f148.jpg     20285\n",
            "The generated caption of 2nd image: a man in a black jacket and black pants is walking down the sidewalk\n",
            "The 0 reference caption of 2nd image: a bunch of people at a train station\n",
            "BLEU score with 0 reference caption: 0.19117647058823528\n",
            "The 1 reference caption of 2nd image: a photo of a train platform on a chilly day\n",
            "BLEU score with 1 reference caption: 0.17647058823529413\n",
            "The 2 reference caption of 2nd image: a train station says tychy miasto\n",
            "BLEU score with 2 reference caption: 0.14705882352941177\n",
            "The 3 reference caption of 2nd image: people are boarding two yellow and blue trains\n",
            "BLEU score with 3 reference caption: 0.20588235294117646\n",
            "The 4 reference caption of 2nd image: several people waiting outside a train station\n",
            "BLEU score with 4 reference caption: 0.19117647058823528\n",
            "The 5 reference caption of 2nd image: two girls take in the view\n",
            "BLEU score with 5 reference caption: 0.19117647058823528\n",
            "done for model: 5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-8p9J1aDkFB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30611b6e-6aca-454b-a73b-95fe2948b9e2"
      },
      "source": [
        "bleu_score1"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  [0.12500000000000003,\n",
              "   0.12500000000000003,\n",
              "   0.1484375,\n",
              "   0.1328125,\n",
              "   0.1328125,\n",
              "   0.140625,\n",
              "   0.2692307692307693,\n",
              "   0.2692307692307693,\n",
              "   0.30769230769230765,\n",
              "   0.28846153846153844,\n",
              "   0.28846153846153844,\n",
              "   0.28846153846153844,\n",
              "   0.44,\n",
              "   0.44,\n",
              "   0.48,\n",
              "   0.44,\n",
              "   0.44,\n",
              "   0.4,\n",
              "   0.29411764705882354,\n",
              "   0.3137254901960784,\n",
              "   0.3333333333333333,\n",
              "   0.3137254901960784,\n",
              "   0.3137254901960784,\n",
              "   0.3137254901960784,\n",
              "   0.2954545454545455,\n",
              "   0.3409090909090909,\n",
              "   0.3409090909090909,\n",
              "   0.3181818181818182,\n",
              "   0.3181818181818182,\n",
              "   0.3181818181818182,\n",
              "   0.34146341463414637,\n",
              "   0.2926829268292683,\n",
              "   0.3902439024390244,\n",
              "   0.3658536585365854,\n",
              "   0.3658536585365854,\n",
              "   0.34146341463414637]),\n",
              " (1,\n",
              "  [0.12500000000000003,\n",
              "   0.12500000000000003,\n",
              "   0.1484375,\n",
              "   0.1328125,\n",
              "   0.1328125,\n",
              "   0.140625,\n",
              "   0.2692307692307693,\n",
              "   0.2692307692307693,\n",
              "   0.30769230769230765,\n",
              "   0.28846153846153844,\n",
              "   0.28846153846153844,\n",
              "   0.28846153846153844,\n",
              "   0.44,\n",
              "   0.44,\n",
              "   0.48,\n",
              "   0.44,\n",
              "   0.44,\n",
              "   0.4,\n",
              "   0.29411764705882354,\n",
              "   0.3137254901960784,\n",
              "   0.3333333333333333,\n",
              "   0.3137254901960784,\n",
              "   0.3137254901960784,\n",
              "   0.3137254901960784,\n",
              "   0.2954545454545455,\n",
              "   0.3409090909090909,\n",
              "   0.3409090909090909,\n",
              "   0.3181818181818182,\n",
              "   0.3181818181818182,\n",
              "   0.3181818181818182,\n",
              "   0.34146341463414637,\n",
              "   0.2926829268292683,\n",
              "   0.3902439024390244,\n",
              "   0.3658536585365854,\n",
              "   0.3658536585365854,\n",
              "   0.34146341463414637]),\n",
              " (2,\n",
              "  [0.12500000000000003,\n",
              "   0.12500000000000003,\n",
              "   0.1484375,\n",
              "   0.1328125,\n",
              "   0.1328125,\n",
              "   0.140625,\n",
              "   0.2692307692307693,\n",
              "   0.2692307692307693,\n",
              "   0.30769230769230765,\n",
              "   0.28846153846153844,\n",
              "   0.28846153846153844,\n",
              "   0.28846153846153844,\n",
              "   0.44,\n",
              "   0.44,\n",
              "   0.48,\n",
              "   0.44,\n",
              "   0.44,\n",
              "   0.4,\n",
              "   0.29411764705882354,\n",
              "   0.3137254901960784,\n",
              "   0.3333333333333333,\n",
              "   0.3137254901960784,\n",
              "   0.3137254901960784,\n",
              "   0.3137254901960784,\n",
              "   0.2954545454545455,\n",
              "   0.3409090909090909,\n",
              "   0.3409090909090909,\n",
              "   0.3181818181818182,\n",
              "   0.3181818181818182,\n",
              "   0.3181818181818182,\n",
              "   0.34146341463414637,\n",
              "   0.2926829268292683,\n",
              "   0.3902439024390244,\n",
              "   0.3658536585365854,\n",
              "   0.3658536585365854,\n",
              "   0.34146341463414637]),\n",
              " (3,\n",
              "  [0.12500000000000003,\n",
              "   0.12500000000000003,\n",
              "   0.1484375,\n",
              "   0.1328125,\n",
              "   0.1328125,\n",
              "   0.140625,\n",
              "   0.2692307692307693,\n",
              "   0.2692307692307693,\n",
              "   0.30769230769230765,\n",
              "   0.28846153846153844,\n",
              "   0.28846153846153844,\n",
              "   0.28846153846153844,\n",
              "   0.44,\n",
              "   0.44,\n",
              "   0.48,\n",
              "   0.44,\n",
              "   0.44,\n",
              "   0.4,\n",
              "   0.29411764705882354,\n",
              "   0.3137254901960784,\n",
              "   0.3333333333333333,\n",
              "   0.3137254901960784,\n",
              "   0.3137254901960784,\n",
              "   0.3137254901960784,\n",
              "   0.2954545454545455,\n",
              "   0.3409090909090909,\n",
              "   0.3409090909090909,\n",
              "   0.3181818181818182,\n",
              "   0.3181818181818182,\n",
              "   0.3181818181818182,\n",
              "   0.34146341463414637,\n",
              "   0.2926829268292683,\n",
              "   0.3902439024390244,\n",
              "   0.3658536585365854,\n",
              "   0.3658536585365854,\n",
              "   0.34146341463414637]),\n",
              " (4,\n",
              "  [0.12500000000000003,\n",
              "   0.12500000000000003,\n",
              "   0.1484375,\n",
              "   0.1328125,\n",
              "   0.1328125,\n",
              "   0.140625,\n",
              "   0.2692307692307693,\n",
              "   0.2692307692307693,\n",
              "   0.30769230769230765,\n",
              "   0.28846153846153844,\n",
              "   0.28846153846153844,\n",
              "   0.28846153846153844,\n",
              "   0.44,\n",
              "   0.44,\n",
              "   0.48,\n",
              "   0.44,\n",
              "   0.44,\n",
              "   0.4,\n",
              "   0.29411764705882354,\n",
              "   0.3137254901960784,\n",
              "   0.3333333333333333,\n",
              "   0.3137254901960784,\n",
              "   0.3137254901960784,\n",
              "   0.3137254901960784,\n",
              "   0.2954545454545455,\n",
              "   0.3409090909090909,\n",
              "   0.3409090909090909,\n",
              "   0.3181818181818182,\n",
              "   0.3181818181818182,\n",
              "   0.3181818181818182,\n",
              "   0.34146341463414637,\n",
              "   0.2926829268292683,\n",
              "   0.3902439024390244,\n",
              "   0.3658536585365854,\n",
              "   0.3658536585365854,\n",
              "   0.34146341463414637]),\n",
              " (5,\n",
              "  [0.12500000000000003,\n",
              "   0.12500000000000003,\n",
              "   0.1484375,\n",
              "   0.1328125,\n",
              "   0.1328125,\n",
              "   0.140625,\n",
              "   0.2692307692307693,\n",
              "   0.2692307692307693,\n",
              "   0.30769230769230765,\n",
              "   0.28846153846153844,\n",
              "   0.28846153846153844,\n",
              "   0.28846153846153844,\n",
              "   0.44,\n",
              "   0.44,\n",
              "   0.48,\n",
              "   0.44,\n",
              "   0.44,\n",
              "   0.4,\n",
              "   0.29411764705882354,\n",
              "   0.3137254901960784,\n",
              "   0.3333333333333333,\n",
              "   0.3137254901960784,\n",
              "   0.3137254901960784,\n",
              "   0.3137254901960784,\n",
              "   0.2954545454545455,\n",
              "   0.3409090909090909,\n",
              "   0.3409090909090909,\n",
              "   0.3181818181818182,\n",
              "   0.3181818181818182,\n",
              "   0.3181818181818182,\n",
              "   0.34146341463414637,\n",
              "   0.2926829268292683,\n",
              "   0.3902439024390244,\n",
              "   0.3658536585365854,\n",
              "   0.3658536585365854,\n",
              "   0.34146341463414637])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guQZ7FWmDkOJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bf0df2db-8306-4bf5-f665-e3b7f64ac0ae"
      },
      "source": [
        "bleu_score2"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0,\n",
              "  [0.13333333333333333,\n",
              "   0.12500000000000003,\n",
              "   0.10000000000000002,\n",
              "   0.14166666666666666,\n",
              "   0.13333333333333333,\n",
              "   0.12500000000000003,\n",
              "   0.23333333333333334,\n",
              "   0.2,\n",
              "   0.15,\n",
              "   0.25,\n",
              "   0.25,\n",
              "   0.2,\n",
              "   0.1780821917808219,\n",
              "   0.1643835616438356,\n",
              "   0.136986301369863,\n",
              "   0.1780821917808219,\n",
              "   0.1643835616438356,\n",
              "   0.1643835616438356,\n",
              "   0.2641509433962264,\n",
              "   0.20754716981132076,\n",
              "   0.1509433962264151,\n",
              "   0.28301886792452824,\n",
              "   0.2641509433962264,\n",
              "   0.20754716981132076,\n",
              "   0.2692307692307693,\n",
              "   0.21153846153846154,\n",
              "   0.15384615384615385,\n",
              "   0.28846153846153844,\n",
              "   0.2692307692307693,\n",
              "   0.21153846153846154,\n",
              "   0.19117647058823528,\n",
              "   0.17647058823529413,\n",
              "   0.14705882352941177,\n",
              "   0.20588235294117646,\n",
              "   0.19117647058823528,\n",
              "   0.19117647058823528]),\n",
              " (1,\n",
              "  [0.13333333333333333,\n",
              "   0.12500000000000003,\n",
              "   0.10000000000000002,\n",
              "   0.14166666666666666,\n",
              "   0.13333333333333333,\n",
              "   0.12500000000000003,\n",
              "   0.23333333333333334,\n",
              "   0.2,\n",
              "   0.15,\n",
              "   0.25,\n",
              "   0.25,\n",
              "   0.2,\n",
              "   0.1780821917808219,\n",
              "   0.1643835616438356,\n",
              "   0.136986301369863,\n",
              "   0.1780821917808219,\n",
              "   0.1643835616438356,\n",
              "   0.1643835616438356,\n",
              "   0.2641509433962264,\n",
              "   0.20754716981132076,\n",
              "   0.1509433962264151,\n",
              "   0.28301886792452824,\n",
              "   0.2641509433962264,\n",
              "   0.20754716981132076,\n",
              "   0.2692307692307693,\n",
              "   0.21153846153846154,\n",
              "   0.15384615384615385,\n",
              "   0.28846153846153844,\n",
              "   0.2692307692307693,\n",
              "   0.21153846153846154,\n",
              "   0.19117647058823528,\n",
              "   0.17647058823529413,\n",
              "   0.14705882352941177,\n",
              "   0.20588235294117646,\n",
              "   0.19117647058823528,\n",
              "   0.19117647058823528]),\n",
              " (2,\n",
              "  [0.13333333333333333,\n",
              "   0.12500000000000003,\n",
              "   0.10000000000000002,\n",
              "   0.14166666666666666,\n",
              "   0.13333333333333333,\n",
              "   0.12500000000000003,\n",
              "   0.23333333333333334,\n",
              "   0.2,\n",
              "   0.15,\n",
              "   0.25,\n",
              "   0.25,\n",
              "   0.2,\n",
              "   0.1780821917808219,\n",
              "   0.1643835616438356,\n",
              "   0.136986301369863,\n",
              "   0.1780821917808219,\n",
              "   0.1643835616438356,\n",
              "   0.1643835616438356,\n",
              "   0.2641509433962264,\n",
              "   0.20754716981132076,\n",
              "   0.1509433962264151,\n",
              "   0.28301886792452824,\n",
              "   0.2641509433962264,\n",
              "   0.20754716981132076,\n",
              "   0.2692307692307693,\n",
              "   0.21153846153846154,\n",
              "   0.15384615384615385,\n",
              "   0.28846153846153844,\n",
              "   0.2692307692307693,\n",
              "   0.21153846153846154,\n",
              "   0.19117647058823528,\n",
              "   0.17647058823529413,\n",
              "   0.14705882352941177,\n",
              "   0.20588235294117646,\n",
              "   0.19117647058823528,\n",
              "   0.19117647058823528]),\n",
              " (3,\n",
              "  [0.13333333333333333,\n",
              "   0.12500000000000003,\n",
              "   0.10000000000000002,\n",
              "   0.14166666666666666,\n",
              "   0.13333333333333333,\n",
              "   0.12500000000000003,\n",
              "   0.23333333333333334,\n",
              "   0.2,\n",
              "   0.15,\n",
              "   0.25,\n",
              "   0.25,\n",
              "   0.2,\n",
              "   0.1780821917808219,\n",
              "   0.1643835616438356,\n",
              "   0.136986301369863,\n",
              "   0.1780821917808219,\n",
              "   0.1643835616438356,\n",
              "   0.1643835616438356,\n",
              "   0.2641509433962264,\n",
              "   0.20754716981132076,\n",
              "   0.1509433962264151,\n",
              "   0.28301886792452824,\n",
              "   0.2641509433962264,\n",
              "   0.20754716981132076,\n",
              "   0.2692307692307693,\n",
              "   0.21153846153846154,\n",
              "   0.15384615384615385,\n",
              "   0.28846153846153844,\n",
              "   0.2692307692307693,\n",
              "   0.21153846153846154,\n",
              "   0.19117647058823528,\n",
              "   0.17647058823529413,\n",
              "   0.14705882352941177,\n",
              "   0.20588235294117646,\n",
              "   0.19117647058823528,\n",
              "   0.19117647058823528]),\n",
              " (4,\n",
              "  [0.13333333333333333,\n",
              "   0.12500000000000003,\n",
              "   0.10000000000000002,\n",
              "   0.14166666666666666,\n",
              "   0.13333333333333333,\n",
              "   0.12500000000000003,\n",
              "   0.23333333333333334,\n",
              "   0.2,\n",
              "   0.15,\n",
              "   0.25,\n",
              "   0.25,\n",
              "   0.2,\n",
              "   0.1780821917808219,\n",
              "   0.1643835616438356,\n",
              "   0.136986301369863,\n",
              "   0.1780821917808219,\n",
              "   0.1643835616438356,\n",
              "   0.1643835616438356,\n",
              "   0.2641509433962264,\n",
              "   0.20754716981132076,\n",
              "   0.1509433962264151,\n",
              "   0.28301886792452824,\n",
              "   0.2641509433962264,\n",
              "   0.20754716981132076,\n",
              "   0.2692307692307693,\n",
              "   0.21153846153846154,\n",
              "   0.15384615384615385,\n",
              "   0.28846153846153844,\n",
              "   0.2692307692307693,\n",
              "   0.21153846153846154,\n",
              "   0.19117647058823528,\n",
              "   0.17647058823529413,\n",
              "   0.14705882352941177,\n",
              "   0.20588235294117646,\n",
              "   0.19117647058823528,\n",
              "   0.19117647058823528]),\n",
              " (5,\n",
              "  [0.13333333333333333,\n",
              "   0.12500000000000003,\n",
              "   0.10000000000000002,\n",
              "   0.14166666666666666,\n",
              "   0.13333333333333333,\n",
              "   0.12500000000000003,\n",
              "   0.23333333333333334,\n",
              "   0.2,\n",
              "   0.15,\n",
              "   0.25,\n",
              "   0.25,\n",
              "   0.2,\n",
              "   0.1780821917808219,\n",
              "   0.1643835616438356,\n",
              "   0.136986301369863,\n",
              "   0.1780821917808219,\n",
              "   0.1643835616438356,\n",
              "   0.1643835616438356,\n",
              "   0.2641509433962264,\n",
              "   0.20754716981132076,\n",
              "   0.1509433962264151,\n",
              "   0.28301886792452824,\n",
              "   0.2641509433962264,\n",
              "   0.20754716981132076,\n",
              "   0.2692307692307693,\n",
              "   0.21153846153846154,\n",
              "   0.15384615384615385,\n",
              "   0.28846153846153844,\n",
              "   0.2692307692307693,\n",
              "   0.21153846153846154,\n",
              "   0.19117647058823528,\n",
              "   0.17647058823529413,\n",
              "   0.14705882352941177,\n",
              "   0.20588235294117646,\n",
              "   0.19117647058823528,\n",
              "   0.19117647058823528])]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    }
  ]
}