{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Using_Word2Vec_embeddings.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HurleyJames/GoogleColabExercise/blob/master/Using_Word2Vec_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtYLDuNmwnnv",
        "colab_type": "text"
      },
      "source": [
        "# News Category Classification using Word2Vec embeddings\n",
        "\n",
        "University of Leeds\n",
        "\n",
        "COMP5623 Artificial Intelligence\n",
        "\n",
        "---\n",
        "\n",
        "We will use two Python libraries:\n",
        "\n",
        "1. **sklearn** a machine learning library\n",
        "\n",
        "2. **Gensim** is a library for unsupervised topic modeling and natural language processing, using modern statistical machine learning\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ElMkuzUm7sOR",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "# 1. Dataset preparation - 20 Newsgroups\n",
        "\n",
        "We will use sklearn to download **20 Newsgroups** (http://qwone.com/~jason/20Newsgroups/), a public available dataset of approximately 20,000 newsgroup posts, partitioned across 20 different newsgroups.  We will only load 3 categories for this example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OAFHvAKcwyH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "categories = [\n",
        "      'comp.graphics',\n",
        "      'sci.med',\n",
        "      'rec.sport.baseball'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAFCFVY_yiCD",
        "colab_type": "text"
      },
      "source": [
        "Load the subset of the 20 Newsgroups dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD84pKuGwxKF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "train_set = fetch_20newsgroups(\n",
        "    subset='train',\n",
        "    categories=categories,\n",
        "    shuffle=True,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlWIPMyey2DF",
        "colab_type": "text"
      },
      "source": [
        "Look at some sample data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWYaiPTKy5sR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_set.target_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7nLorBMe5iJC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Total number of news articles:\", len(train_set.data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gciDCHgE5tyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"\\n\".join(train_set.data[0].split(\"\\n\")[:10]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiQMJBoS55J4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_set.target_names[train_set.target[0]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nm2WQXeq7wee",
        "colab_type": "text"
      },
      "source": [
        "# 2. Training a Word2Vec model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wqlR8EQVyxlo",
        "colab_type": "text"
      },
      "source": [
        "Now we will train a Word2Vec model which we will use to map each word in each news article to a feature representation. While the best models are trained on very large amounts of data, due to resources, we will use a model trained on this small corpus.\n",
        "\n",
        "First we pre-process the data into lists of words."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7LUiNMJyjmQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parsed_train_data = []\n",
        "for article in train_set.data:\n",
        "  parsed_train_data.append(article.replace('\\n',' ').split())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSLO2Un5-6rN",
        "colab_type": "text"
      },
      "source": [
        "Train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AnxiOTQzxbe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "feature_length = 50\n",
        "\n",
        "word2vec_model = Word2Vec(\n",
        "    sentences=parsed_train_data,\n",
        "    window=5,\n",
        "    sg=1,       # Use skip-gram\n",
        "    size=feature_length\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtoED80r_VNq",
        "colab_type": "text"
      },
      "source": [
        "How good is it? Sanity check..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVfIoTa0_ZsK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vec_model.most_similar(\"disease\", topn=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4DAo_PMyz6em",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vec_model.most_similar(\"baseball\", topn=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XktqzfRhAEtf",
        "colab_type": "text"
      },
      "source": [
        "What is the feature representation for one word?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRznixyfAH8X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word2vec_model[\"baseball\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9axD6V5_3Bh",
        "colab_type": "text"
      },
      "source": [
        "# 3. Performing classification on news articles using feature embeddings\n",
        "\n",
        "Finally, we will represent each news article as a block of word features, and perform classification on the embedded representations.\n",
        "\n",
        "As an over-simplification of the problem (for the purposes of illustration), we will choose the N first words to represent an article so that all our article sizes are fixed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P9WJeqvnE_Ph",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "article_size = 13 * feature_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWYY3zHXIUkt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def embed_article(article, cutoff):\n",
        "  # Save the feature representation for each word in the article\n",
        "  embedded_article = []\n",
        "  for word in article:\n",
        "    try:\n",
        "      embedded_article.append(word2vec_model[word])\n",
        "    except(KeyError): # Ignore words not in the model vocabulary\n",
        "      pass\n",
        "  return np.array(embedded_article).flatten()[:cutoff]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bvjf7Ow1AzJs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedded_train_set = []\n",
        "\n",
        "for article in parsed_train_data:\n",
        "  embedded_train_set.append(embed_article(article, article_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1w5UV_V6B2Xw",
        "colab_type": "text"
      },
      "source": [
        "Now we can try training a simple linear classifier to classify the news articles into their categories. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wN4G1K_bBS_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZWDSn8ZB0Kn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linear_classifier = SGDClassifier()\n",
        "\n",
        "linear_classifier.fit(\n",
        "      embedded_train_set,\n",
        "      train_set.target,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mim8hOp5Gy5R",
        "colab_type": "text"
      },
      "source": [
        "Can it predict on new articles?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G5MmRfMDEMhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "new_articles = [\n",
        "      # Article one - from latest NVidia news\n",
        "      \"Nvidia RTX 2080 Ti Cyberpunk 2077 GPU is revealed – but you can’t buy one.\" +\n",
        "      \"77 limited edition graphics cards are to be given away in a competition\" +\n",
        "      \"The mysterious Cyberpunk 2077-themed GPU Nvidia recently teased has been revealed,\" +\n",
        "      \"and the reality of the graphics card may be a touch disappointing for some folks, in that\" +\n",
        "      \"it isn’t a new model – and you won’t be able to buy one.\" +\n",
        "      \"The card is simply a GeForce RTX 2080 Ti (and appears to be exactly the same model,\" + \n",
        "      \"and shroud design) decked out with the Cyberpunk 2077 colors and logo, which admittedly\" +\n",
        "      \"looks pretty cool, but isn’t the GeForce RTX 2080 Ti Super\" +\n",
        "      \"AMD confirms ‘Nvidia killer’ graphics card will be out in 2020\",\n",
        "      # Article two - from Health -> Oncology\n",
        "      \"Breast cancer test could predict chances of disease return 20 years later, study shows\" +\n",
        "      \"Molecular nature of a woman’s breast cancer determines how their disease could progress,\" +\n",
        "      \"not just for the first five years, but also later,' says researcher\" +\n",
        "      \"A new test could identify breast cancers that are likely to return more than 20 years later\" +\n",
        "      \"development that might herald an era of personalised medicine.\" +\n",
        "      \"The way a patient’s cancer will progress can be determined by categorising molecular and\" +\n",
        "      \"genetic markers of breast tumours into 11 subtypes, University of Cambridge researchers found.\" +  \n",
        "      \"Following around 2,000 women over 20 years, the team funded by the Cancer Research charity found\" +\n",
        "      \"some women with initially aggressive cancers had a low chance of tumours returning after five years.\"\n",
        "]\n",
        "\n",
        "# Parse and embed\n",
        "parsed_na = [a.replace('\\n',' ').split() for a in new_articles]\n",
        "embedded_new_articles = [embed_article(a, article_size) for a in parsed_na]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4hudi7IQIAIB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predicted = linear_classifier.predict(embedded_new_articles)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUIjj0DrKEfQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i, category in enumerate(predicted):\n",
        "  print(\"New article\", i, \" predicted cateogry =>\", train_set.target_names[category])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5XJkagZKcCr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}